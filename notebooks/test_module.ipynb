{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600501028029",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "with path.Path('..'):\n",
    "    from services.theme_extractor.preprocessing import ArticlePreprocessJob, ArticlePreprocessor \n",
    "\n",
    "    from services.theme_extractor.wv_model import  WVModelBuilder\n",
    "\n",
    "    from services.theme_extractor.clustering import Clusterer\n",
    "\n",
    "    from services.theme_extractor.keyword_extraction import KeywordExtractor\n",
    "\n",
    "    from services.libs.data_model import ProcessedArticle, Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dfd2c33b-bdb8-456d-bdff-b1e1ab014309\n"
    }
   ],
   "source": [
    "\n",
    "apj = ArticlePreprocessJob()\n",
    "\n",
    "load_id = apj.get_latest_article_load().id\n",
    "print(load_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10000"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "articles = apj.get_articles_for_load(load_id, max_articles=10000)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "\n",
    "class APRun:\n",
    "\n",
    "    processed_articles: List[ProcessedArticle]\n",
    "\n",
    "    model: Doc2Vec\n",
    "\n",
    "    labels: np.array\n",
    "\n",
    "    clusters: List[Theme]\n",
    "\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, name, steps=['lemmatize', 'postag', 'phrasing'], postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "        self.name = name\n",
    "        self.steps = steps;\n",
    "        self.postags = postags;\n",
    "        # self.processed_articles = processed_articles\n",
    "        # self.model = model\n",
    "        # self.labels = labels\n",
    "        # self.clusters = clusters\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "UMAP(a=None, angular_rp_forest=False, b=None, init='spectral',\n     learning_rate=1.0, local_connectivity=1.0, metric='cosine',\n     metric_kwds=None, min_dist=0.1, n_components=100, n_epochs=None,\n     n_neighbors=15, negative_sample_rate=5, random_state=666,\n     repulsion_strength=1.0, set_op_mix_ratio=1.0, spread=1.0,\n     target_metric='categorical', target_metric_kwds=None,\n     target_n_neighbors=-1, target_weight=0.5, transform_queue_size=4.0,\n     transform_seed=42, verbose=True)\nConstruct fuzzy simplicial set\nSat Sep 19 19:11:32 2020 Finding Nearest Neighbors\nSat Sep 19 19:11:32 2020 Building RP forest with 10 trees\nSat Sep 19 19:11:33 2020 NN descent for 13 iterations\n\t 0  /  13\n\t 1  /  13\n\t 2  /  13\n\t 3  /  13\n\t 4  /  13\nSat Sep 19 19:11:39 2020 Finished Nearest Neighbor Search\nSat Sep 19 19:11:40 2020 Construct embedding\n\tcompleted  0  /  500 epochs\n\tcompleted  50  /  500 epochs\n\tcompleted  100  /  500 epochs\n\tcompleted  150  /  500 epochs\n\tcompleted  200  /  500 epochs\n\tcompleted  250  /  500 epochs\n\tcompleted  300  /  500 epochs\n\tcompleted  350  /  500 epochs\n\tcompleted  400  /  500 epochs\n\tcompleted  450  /  500 epochs\nSat Sep 19 19:12:54 2020 Finished embedding\nExtracting keywords for 807 themes\nUMAP(a=None, angular_rp_forest=False, b=None, init='spectral',\n     learning_rate=1.0, local_connectivity=1.0, metric='cosine',\n     metric_kwds=None, min_dist=0.1, n_components=100, n_epochs=None,\n     n_neighbors=15, negative_sample_rate=5, random_state=666,\n     repulsion_strength=1.0, set_op_mix_ratio=1.0, spread=1.0,\n     target_metric='categorical', target_metric_kwds=None,\n     target_n_neighbors=-1, target_weight=0.5, transform_queue_size=4.0,\n     transform_seed=42, verbose=True)\nConstruct fuzzy simplicial set\nSat Sep 19 19:38:59 2020 Finding Nearest Neighbors\nSat Sep 19 19:38:59 2020 Building RP forest with 10 trees\nSat Sep 19 19:38:59 2020 NN descent for 13 iterations\n\t 0  /  13\n\t 1  /  13\n\t 2  /  13\n\t 3  /  13\n\t 4  /  13\n\t 5  /  13\nSat Sep 19 19:39:05 2020 Finished Nearest Neighbor Search\nSat Sep 19 19:39:06 2020 Construct embedding\n\tcompleted  0  /  500 epochs\n\tcompleted  50  /  500 epochs\n\tcompleted  100  /  500 epochs\n\tcompleted  150  /  500 epochs\n\tcompleted  200  /  500 epochs\n\tcompleted  250  /  500 epochs\n\tcompleted  300  /  500 epochs\n\tcompleted  350  /  500 epochs\n\tcompleted  400  /  500 epochs\n\tcompleted  450  /  500 epochs\nSat Sep 19 19:40:16 2020 Finished embedding\nExtracting keywords for 797 themes\nUMAP(a=None, angular_rp_forest=False, b=None, init='spectral',\n     learning_rate=1.0, local_connectivity=1.0, metric='cosine',\n     metric_kwds=None, min_dist=0.1, n_components=100, n_epochs=None,\n     n_neighbors=15, negative_sample_rate=5, random_state=666,\n     repulsion_strength=1.0, set_op_mix_ratio=1.0, spread=1.0,\n     target_metric='categorical', target_metric_kwds=None,\n     target_n_neighbors=-1, target_weight=0.5, transform_queue_size=4.0,\n     transform_seed=42, verbose=True)\nConstruct fuzzy simplicial set\nSat Sep 19 20:19:44 2020 Finding Nearest Neighbors\nSat Sep 19 20:19:44 2020 Building RP forest with 10 trees\nSat Sep 19 20:19:44 2020 NN descent for 13 iterations\n\t 0  /  13\n\t 1  /  13\n\t 2  /  13\n\t 3  /  13\n\t 4  /  13\n\t 5  /  13\nSat Sep 19 20:19:50 2020 Finished Nearest Neighbor Search\nSat Sep 19 20:19:50 2020 Construct embedding\n\tcompleted  0  /  500 epochs\n\tcompleted  50  /  500 epochs\n\tcompleted  100  /  500 epochs\n\tcompleted  150  /  500 epochs\n\tcompleted  200  /  500 epochs\n\tcompleted  250  /  500 epochs\n\tcompleted  300  /  500 epochs\n\tcompleted  350  /  500 epochs\n\tcompleted  400  /  500 epochs\n\tcompleted  450  /  500 epochs\nSat Sep 19 20:20:57 2020 Finished embedding\nExtracting keywords for 810 themes\nUMAP(a=None, angular_rp_forest=False, b=None, init='spectral',\n     learning_rate=1.0, local_connectivity=1.0, metric='cosine',\n     metric_kwds=None, min_dist=0.1, n_components=100, n_epochs=None,\n     n_neighbors=15, negative_sample_rate=5, random_state=666,\n     repulsion_strength=1.0, set_op_mix_ratio=1.0, spread=1.0,\n     target_metric='categorical', target_metric_kwds=None,\n     target_n_neighbors=-1, target_weight=0.5, transform_queue_size=4.0,\n     transform_seed=42, verbose=True)\nConstruct fuzzy simplicial set\nSat Sep 19 20:47:02 2020 Finding Nearest Neighbors\nSat Sep 19 20:47:02 2020 Building RP forest with 10 trees\nSat Sep 19 20:47:02 2020 NN descent for 13 iterations\n\t 0  /  13\n\t 1  /  13\n\t 2  /  13\n\t 3  /  13\n\t 4  /  13\nSat Sep 19 20:47:07 2020 Finished Nearest Neighbor Search\nSat Sep 19 20:47:08 2020 Construct embedding\n\tcompleted  0  /  500 epochs\n\tcompleted  50  /  500 epochs\n\tcompleted  100  /  500 epochs\n\tcompleted  150  /  500 epochs\n\tcompleted  200  /  500 epochs\n\tcompleted  250  /  500 epochs\n\tcompleted  300  /  500 epochs\n\tcompleted  350  /  500 epochs\n\tcompleted  400  /  500 epochs\n\tcompleted  450  /  500 epochs\nSat Sep 19 20:48:12 2020 Finished embedding\nExtracting keywords for 815 themes\nUMAP(a=None, angular_rp_forest=False, b=None, init='spectral',\n     learning_rate=1.0, local_connectivity=1.0, metric='cosine',\n     metric_kwds=None, min_dist=0.1, n_components=100, n_epochs=None,\n     n_neighbors=15, negative_sample_rate=5, random_state=666,\n     repulsion_strength=1.0, set_op_mix_ratio=1.0, spread=1.0,\n     target_metric='categorical', target_metric_kwds=None,\n     target_n_neighbors=-1, target_weight=0.5, transform_queue_size=4.0,\n     transform_seed=42, verbose=True)\nConstruct fuzzy simplicial set\nSat Sep 19 21:12:20 2020 Finding Nearest Neighbors\nSat Sep 19 21:12:20 2020 Building RP forest with 10 trees\nSat Sep 19 21:12:21 2020 NN descent for 13 iterations\n\t 0  /  13\n\t 1  /  13\n\t 2  /  13\n\t 3  /  13\n\t 4  /  13\nSat Sep 19 21:12:27 2020 Finished Nearest Neighbor Search\nSat Sep 19 21:12:28 2020 Construct embedding\n\tcompleted  0  /  500 epochs\n\tcompleted  50  /  500 epochs\n\tcompleted  100  /  500 epochs\n\tcompleted  150  /  500 epochs\n\tcompleted  200  /  500 epochs\n\tcompleted  250  /  500 epochs\n\tcompleted  300  /  500 epochs\n\tcompleted  350  /  500 epochs\n\tcompleted  400  /  500 epochs\n\tcompleted  450  /  500 epochs\nSat Sep 19 21:13:32 2020 Finished embedding\nExtracting keywords for 833 themes\n"
    }
   ],
   "source": [
    "def run_ap_options(run_options: APRun):\n",
    "\n",
    "    load_id = run_options.name\n",
    "\n",
    "    ap = ArticlePreprocessor(steps=run_options.steps, allowed_postags=run_options.postags)\n",
    "    processed_articles = ap.preprocess_articles(articles, load_id)\n",
    "\n",
    "    model_builder = WVModelBuilder()\n",
    "\n",
    "    model = model_builder.build_wv_model(processed_articles)\n",
    "\n",
    "    labels = Clusterer(model, processed_articles, load_id).create_mapping(min_cluster_size=3, cluster_selection_epsilon=0.1)\n",
    "    \n",
    "    clusters = KeywordExtractor(model).create_themes(load_id, processed_articles, labels)\n",
    "\n",
    "    return processed_articles, model, labels, clusters\n",
    "\n",
    "def calculate_runs():\n",
    "\n",
    "    runs: List[APRun] = [\n",
    "        APRun('run_with_all'),\n",
    "        APRun('run_with_none', steps=[]),\n",
    "        APRun('run_with_no_phrasing', steps=['postag', 'lemmatize']),\n",
    "        APRun('run_with_no_lemmatize', steps=['postag', 'phrasing']),\n",
    "        APRun('run_with_no_postag', steps=['lemmatize', 'phrasing'])\n",
    "\n",
    "    ]\n",
    "\n",
    "    for run in runs:\n",
    "        processed_articles, model, labels, clusters = run_ap_options(run);\n",
    "        run.processed_articles = processed_articles\n",
    "        run.model = model\n",
    "        run.labels = labels\n",
    "        run.clusters = clusters\n",
    "\n",
    "    return runs\n",
    "    \n",
    "ap_runs = calculate_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\noff pension\n21\n['fall flat pension row', 'showdown Macron pension', 'Macron pension', 'France brace gilet', 'magic end standoff pension', 'Paris pension row deepen', 'standoff pension', 'cut power pension', 'presidential pension']\n13\nVaporfly shoe\n5\n['revolutionise marathon', 'Nike Vaporfly shoe', 'Nike shoe', 'Nike Vaporflys', 'Nike shoe revolutionise', 'Nike Vaporfly', 'shoe regulation', 'Nike Vaporflys escape ban', 'Vaporflys escape ban running']\n20\nCain emotionally\n5\n['callous insensitive pressure female', 'insensitive pressure female', 'stage protest', 'sorry callous insensitive pressure', 'physical abuse Oregon', 'callous insensitive pressure', 'pressure female', 'insensitive pressure', 'Salazar say Mo']\n499\ndefine extinction rebellion week long\n3\n['xr autumn', 'xr autumn uprise', 'section order xr', 'say xr', 'met embroil grow controversy', 'say xr autumn', 'metropolitan police sadiq extremely concerned', 'xr must now cease protest', 'bring behalf xr']\n599\nbrand Mulberry\n9\n['Deliveroo could lead high price say', 'tycoon rescue Aston', 'fashion tycoon', 'buyout ever say', 'Deliveroo could lead high price', 'buyout ever', 'could lead high price say CMA', 'snap stake luxury', 'Group snap stake luxury']\n"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(ap_runs[0].labels)\n",
    "\n",
    "for i in [1, 14, 21, 500, 600]:\n",
    "    cluster = ap_runs[0].clusters[i]\n",
    "    print(cluster.id)\n",
    "    print(cluster.name)\n",
    "    print(counts[cluster.id])\n",
    "    print(cluster.theme_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(set1: np.array, set2: np.array):\n",
    "    return len(np.intersect1d(set1, set2)) / len(np.union1d(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThemeTarget:\n",
    "\n",
    "    def __init__(self, label: int, name: str):\n",
    "        self.label = label\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "targets: List[ThemeTarget] = [\n",
    "    ThemeTarget(0, 'Grenfell'),\n",
    "    ThemeTarget(13, 'Coronavirus students'),\n",
    "    ThemeTarget(20, 'Shoes'),\n",
    "    ThemeTarget(399, 'Suleimani'),\n",
    "    ThemeTarget(599, 'Labour Manifesto'),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_jaccards(arts):\n",
    "    jaccard_data = {}\n",
    "    for run in ap_runs:\n",
    "        jaccard_data[run.name] = {}\n",
    "        for other_run in ap_runs:\n",
    "            jaccard_data[run.name][other_run.name] = {}\n",
    "            for cluster in run.clusters:\n",
    "                jaccard_score = 1\n",
    "                if run != other_run:\n",
    "                    arr = Counter(other_run.labels[run.labels == cluster.id]).most_common(2)\n",
    "                    if arr[0][0] != -1:\n",
    "                        lab = arr[0][0]\n",
    "                        jaccard_score = jaccard(arts[run.labels == cluster.id], arts[other_run.labels == lab])\n",
    "                    elif len(arr) > 1:\n",
    "                        lab = arr[1][0]\n",
    "                        jaccard_score = jaccard(arts[run.labels == cluster.id], arts[other_run.labels == lab])\n",
    "                    else:\n",
    "                        jaccard_score = 0\n",
    "\n",
    "                jaccard_data[run.name][other_run.name][cluster.id] = jaccard_score\n",
    "    return jaccard_data\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jacc_data = get_jaccards(np.array([art.id for art in articles]))\n"
   ]
  },
  {
   "source": [
    "column_names = []\n",
    "row_names = []\n",
    "data = []\n",
    "for run in jacc_data:\n",
    "    column_names.append(run)\n",
    "    row_names.append(run)\n",
    "    for other_run in jacc_data[run]:\n",
    "        arr = list(jacc_data[run][other_run].values())\n",
    "        data.append(np.median(arr))\n",
    "pd.DataFrame(data = np.reshape(data, (5,5)), columns=column_names)\n",
    "        "
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 143,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   run_with_all  run_with_none  run_with_no_phrasing  run_with_no_lemmatize  \\\n0      1.000000       0.333333              0.400000               0.500000   \n1      0.333333       1.000000              0.333333               0.333333   \n2      0.400000       0.333333              1.000000               0.400000   \n3      0.476190       0.333333              0.400000               1.000000   \n4      0.307692       0.333333              0.285714               0.333333   \n\n   run_with_no_postag  \n0            0.318182  \n1            0.333333  \n2            0.293798  \n3            0.333333  \n4            1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_with_all</th>\n      <th>run_with_none</th>\n      <th>run_with_no_phrasing</th>\n      <th>run_with_no_lemmatize</th>\n      <th>run_with_no_postag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n      <td>0.500000</td>\n      <td>0.318182</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.400000</td>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>0.400000</td>\n      <td>0.293798</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.476190</td>\n      <td>0.333333</td>\n      <td>0.400000</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.307692</td>\n      <td>0.333333</td>\n      <td>0.285714</td>\n      <td>0.333333</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 143
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 13)]\n1.0\n[(2, 13)]\n1.0\n[(0, 13)]\n1.0\n[(0, 13)]\n1.0\n[(72, 13)]\n1.0\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_with_all</th>\n      <th>run_with_none</th>\n      <th>run_with_no_phrasing</th>\n      <th>run_with_no_lemmatize</th>\n      <th>run_with_no_postag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sacoolas immunity</td>\n      <td>as immunity</td>\n      <td>Sacoolas immunity row</td>\n      <td>Dunn twin</td>\n      <td>’s family to</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Harry Dunn twin</td>\n      <td>immunity for envoy</td>\n      <td>immunity row</td>\n      <td>Sacoolas immunity</td>\n      <td>Dunn ’s family</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>family sue US government</td>\n      <td>immunity for</td>\n      <td>lose immunity</td>\n      <td>immunity envoy wife</td>\n      <td>immunity for envoy 's</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>immunity row</td>\n      <td>immunity for envoy 's</td>\n      <td>wife will</td>\n      <td>tearful account find</td>\n      <td>for -PRON- son</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>immunity envoy</td>\n      <td>family to sue</td>\n      <td>immunity envoy wife</td>\n      <td>Harry Dunn twin</td>\n      <td>’s family to travel to US</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>wife leave</td>\n      <td>ask us to waive</td>\n      <td>wife will return</td>\n      <td>son US TV</td>\n      <td>Dunn ’s family to</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>meet Anne Sacoolas immunity</td>\n      <td>as immunity row</td>\n      <td>US seek justice</td>\n      <td>Harry Dunn</td>\n      <td>Dunn 's twin</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Dunn twin</td>\n      <td>wife in</td>\n      <td>immunity row continue</td>\n      <td>Harry Dunn family</td>\n      <td>immunity for</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>family sue US</td>\n      <td>family to travel to</td>\n      <td>family travel</td>\n      <td>meet Anne Sacoolas immunity</td>\n      <td>immunity for envoy</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>wife will return</td>\n      <td>’s family</td>\n      <td>tearful account find</td>\n      <td>next room</td>\n      <td>as immunity</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(13, 5)]\n1.0\n[(57, 4), (-1, 1)]\n0.21052631578947367\n[(45, 5)]\n0.22727272727272727\n[(29, 4), (30, 1)]\n0.18181818181818182\n[(22, 5)]\n0.20833333333333334\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_with_all</th>\n      <th>run_with_none</th>\n      <th>run_with_no_phrasing</th>\n      <th>run_with_no_lemmatize</th>\n      <th>run_with_no_postag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bury FC despair club</td>\n      <td>and football regulation</td>\n      <td>Bury CVA</td>\n      <td>Bury CVA</td>\n      <td>takeover go through without full</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FC despair club expel</td>\n      <td>CVA and football regulation</td>\n      <td>club expel Football</td>\n      <td>FC despair club expel</td>\n      <td>club should bail</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Distressed Bury</td>\n      <td>as club</td>\n      <td>Bury say</td>\n      <td>Bury season opener MK</td>\n      <td>Bury on the brink of</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chain drainpipe save club</td>\n      <td>club be</td>\n      <td>Bury Tuesday</td>\n      <td>despair club expel Football</td>\n      <td>Bury takeover go through without</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>club should bail</td>\n      <td>takeover to go through</td>\n      <td>football regulation</td>\n      <td>Distressed Bury</td>\n      <td>takeover go through without</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>despair club</td>\n      <td>and football</td>\n      <td>CVA football regulation</td>\n      <td>Bury Tuesday</td>\n      <td>spotlight over Pastore</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>drainpipe save club</td>\n      <td>’s expulsion</td>\n      <td>Bury FC</td>\n      <td>Bury FC despair club</td>\n      <td>fear over Bury</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>FC despair club</td>\n      <td>EFL over club ’s</td>\n      <td>chain drainpipe save club</td>\n      <td>Bury FC</td>\n      <td>spotlight over</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>despair club expel Football</td>\n      <td>Campbell takeover to</td>\n      <td>MK don</td>\n      <td>Bury fan</td>\n      <td>call for inquiry into CVA and</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>save club</td>\n      <td>Bury CVA prompt insolvency</td>\n      <td>despair club expel Football</td>\n      <td>drainpipe save club</td>\n      <td>block from</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(20, 9)]\n1.0\n[(41, 9)]\n0.21951219512195122\n[(21, 9)]\n0.6\n[(16, 9)]\n0.6428571428571429\n[(35, 9)]\n0.8181818181818182\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_with_all</th>\n      <th>run_with_none</th>\n      <th>run_with_no_phrasing</th>\n      <th>run_with_no_lemmatize</th>\n      <th>run_with_no_postag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dope say</td>\n      <td>athlete to</td>\n      <td>order athlete</td>\n      <td>Salazar dope</td>\n      <td>’s athlete</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Salazar dope</td>\n      <td>on dope</td>\n      <td>athlete sever link Alberto</td>\n      <td>dope say</td>\n      <td>athlete to</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Salazar athlete</td>\n      <td>order athlete to sever</td>\n      <td>order athlete sever</td>\n      <td>Salazar athlete</td>\n      <td>athlete to sever</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>athlete sever link Alberto</td>\n      <td>’s athlete</td>\n      <td>athlete sever link</td>\n      <td>Salazar insist never mislead</td>\n      <td>athlete to sever all</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>order athlete</td>\n      <td>on dope ,</td>\n      <td>Sebastian Coe order athlete</td>\n      <td>Mo Farah turn medium</td>\n      <td>on dope</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Coe order athlete</td>\n      <td>eager to keep streak</td>\n      <td>athlete sever</td>\n      <td>US sprinter</td>\n      <td>Salazar ’s athlete</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>order athlete sever link</td>\n      <td>olympic success after</td>\n      <td>Coe order athlete</td>\n      <td>sprinter Christian</td>\n      <td>Farah turn on</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>order athlete sever</td>\n      <td>reveal extent of</td>\n      <td>sever link Alberto Salazar</td>\n      <td>Sebastian Coe</td>\n      <td>which lead to Salazar</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>athlete sever</td>\n      <td>record - holder</td>\n      <td>Salazar athlete</td>\n      <td>lead Salazar</td>\n      <td>turn on</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Coe order athlete sever</td>\n      <td>relay silver in</td>\n      <td>dope say</td>\n      <td>Alberto Salazar athlete</td>\n      <td>Alberto Salazar ’s athlete</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(399, 9)]\n1.0\n[(197, 6), (609, 2), (-1, 1)]\n0.6\n[(66, 6), (215, 3)]\n0.42857142857142855\n[(96, 8), (-1, 1)]\n0.6153846153846154\n[(227, 5), (213, 1), (521, 1), (-1, 1), (534, 1)]\n0.35714285714285715\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_with_all</th>\n      <th>run_with_none</th>\n      <th>run_with_no_phrasing</th>\n      <th>run_with_no_lemmatize</th>\n      <th>run_with_no_postag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>stimulus RBA</td>\n      <td>of stimulus</td>\n      <td>underspend NDIS</td>\n      <td>stimulus RBA</td>\n      <td>of stimulus</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hint further cut interest</td>\n      <td>to justify effectiveness of</td>\n      <td>inequality craft good story</td>\n      <td>underspend NDIS</td>\n      <td>to stimulate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>justify effectiveness interest rate</td>\n      <td>growth in</td>\n      <td>labor turn heat</td>\n      <td>economy heap</td>\n      <td>interest rate to</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Deloitte say</td>\n      <td>to stimulate</td>\n      <td>budget effectively balance underspend</td>\n      <td>Labor Jim Chalmers say</td>\n      <td>economy be</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>further cut interest</td>\n      <td>economy be</td>\n      <td>stimulus RBA</td>\n      <td>heap stimulus</td>\n      <td>of 0.75</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>effectiveness interest rate</td>\n      <td>justify effectiveness</td>\n      <td>coalition income</td>\n      <td>further cut interest</td>\n      <td>economic shock , report find</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>shock report find</td>\n      <td>wage cap to stimulate</td>\n      <td>remove buffer economic shock</td>\n      <td>Chalmers say</td>\n      <td>wage cap to stimulate</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>warn Deloitte</td>\n      <td>growth in more than</td>\n      <td>tax cut remove buffer</td>\n      <td>hint further cut interest</td>\n      <td>levy ,</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>economic shock</td>\n      <td>to justify effectiveness</td>\n      <td>low boost weak economy</td>\n      <td>Reserve Bank</td>\n      <td>to justify effectiveness of</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>historic low boost</td>\n      <td>hint at</td>\n      <td>reference worsen wealth</td>\n      <td>justify effectiveness interest rate</td>\n      <td>growth in more than a decade</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(599, 3)]\n1.0\n[(526, 3)]\n1.0\n[(379, 3)]\n0.6\n[(465, 3)]\n1.0\n[(677, 3)]\n1.0\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_with_all</th>\n      <th>run_with_none</th>\n      <th>run_with_no_phrasing</th>\n      <th>run_with_no_lemmatize</th>\n      <th>run_with_no_postag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>morrison government power forward advertising</td>\n      <td>department reject the audit</td>\n      <td>advertising last financial year</td>\n      <td>advertising due complete month</td>\n      <td>spruike -PRON-</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>advertising framework</td>\n      <td>say the campaign</td>\n      <td>government advertising</td>\n      <td>morrison government power forward advertising</td>\n      <td>spend $ 18.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>anao calculate ad</td>\n      <td>on the campaign</td>\n      <td>advertising last financial</td>\n      <td>advertising year average anao</td>\n      <td>-PRON- spend $</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>advertising real measure</td>\n      <td>of the campaign .</td>\n      <td>blitz warning</td>\n      <td>advertising due complete</td>\n      <td>the may election</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>blitz christmas audit find</td>\n      <td>conroy and the</td>\n      <td>taxpayer fund government advertising</td>\n      <td>government advertising due complete</td>\n      <td>blitz spruike</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>forward campaign spruike</td>\n      <td>power forward \" campaign</td>\n      <td>coalition spend</td>\n      <td>blitz spruike</td>\n      <td>conroy and the crossbencher</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>power forward advertising</td>\n      <td>campaign .</td>\n      <td>coalition spend pre election</td>\n      <td>ministership conroy</td>\n      <td>spend $ 14.1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>blitz spruike</td>\n      <td>campaign \" .</td>\n      <td>fund government advertising</td>\n      <td>blitz Christmas</td>\n      <td>annual report spend $</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>just federal election</td>\n      <td>campaign that the</td>\n      <td>staffer hour</td>\n      <td>campaign prove ineffective audit</td>\n      <td>-PRON- spend $ 18.5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>effectively administer advertising</td>\n      <td>campaign in</td>\n      <td>taxpayer pay staffer hour</td>\n      <td>blitz warning</td>\n      <td>spend $ 100.1</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for target in targets:\n",
    "\n",
    "    label_id = target.label\n",
    "    mapping = ap_runs[0].labels\n",
    "    mapping_articles_idx = np.where(mapping == label_id)\n",
    "    base_articles = [a.id for a in np.array(articles)[mapping_articles_idx]]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, run in enumerate(ap_runs):\n",
    "        cluster_match = Counter(run.labels[mapping_articles_idx]).most_common();\n",
    "        main_cluster = cluster_match[0][0]\n",
    "        clus = [c for c in run.clusters if c.id == main_cluster][0]\n",
    "        print(cluster_match)\n",
    "\n",
    "        theme_articles = [a.id for a in np.array(articles)[np.where(run.labels == main_cluster)]]\n",
    "        print(jaccard(theme_articles, base_articles))\n",
    "        data[run.name] = [[c.name] + c.theme_words for c in run.clusters if c.id == main_cluster][0]\n",
    "\n",
    "    display(HTML(pd.DataFrame(data).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}