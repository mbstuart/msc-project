{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598252183613",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import services.theme_extractor_api.main\n",
    "from services.theme_extractor.cluster_job import ClusterJob\n",
    "from services.theme_extractor.wv_model_job import WVModelJob\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvm = WVModelJob()\n",
    "\n",
    "al = wvm.get_latest_article_load()\n",
    "\n",
    "model = wvm.get_model_from_disk(al.id)\n",
    "\n",
    "cj = ClusterJob(model, al.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = cj.filter_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.theme_extractor.clusterer import Clusterer\n",
    "\n",
    "c = Clusterer(model, articles, al.id, from_scratch=True, min_cluster_size=3, cluster_selection_epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, t = c.create_themes_and_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "from services.libs.data_model.processed_article import ProcessedArticle\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from  services.theme_extractor.logger import logger\n",
    "\n",
    "\n",
    "\n",
    "from services.libs.data_model.theme import Theme\n",
    "from services.libs.data_model.article import Article\n",
    "        \n",
    "def __get_class_words_from_doc_selection(docs_in_class: List[ProcessedArticle], vecs, model: Doc2Vec):\n",
    "        doc_dict = {}\n",
    "\n",
    "        for doc in docs_in_class:\n",
    "            for word in doc.words:\n",
    "                if word in doc_dict:\n",
    "                    doc_dict[word] += 1\n",
    "                else:\n",
    "                    doc_dict[word] = 1\n",
    "\n",
    "        d = Counter(doc_dict)\n",
    "\n",
    "        top_words = d.most_common(1000)\n",
    "\n",
    "        word_2_vec_ranking = {}\n",
    "\n",
    "        for word in top_words:\n",
    "            \n",
    "            if(word[0] not in model.wv.vocab):\n",
    "                continue;\n",
    "            \n",
    "            word_vec = model[word[0]]\n",
    "            av_vec = np.average(vecs, axis=0)\n",
    "\n",
    "\n",
    "            similarity = 1 - spatial.distance.cosine(word_vec, av_vec)\n",
    "            word_2_vec_ranking[word[0]] = similarity\n",
    "\n",
    "        rank_counter = Counter(word_2_vec_ranking)\n",
    "\n",
    "        return [w[0] for w in rank_counter.most_common(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "''\n",
    "## votes\n",
    "''\n",
    "def av_run(votes: np.array):\n",
    "    \n",
    "    # Expect votes to be a 2D numpy array where each row is a ballot and each column represents a candidate. \n",
    "    # The value of the cell represents the preference order of that voter - lower = preference!\n",
    "\n",
    "    n_ballots, n_candidates = votes.shape\n",
    "\n",
    "    running=True\n",
    "\n",
    "    n_round = 1\n",
    "\n",
    "    losers = set()\n",
    "\n",
    "    while(running):\n",
    "\n",
    "\n",
    "        winners = []\n",
    "\n",
    "        for ballot in votes:\n",
    "            winners.append(np.where(ballot == np.amin(ballot))[0][0])\n",
    "\n",
    "        vote_counts = Counter(winners).most_common()\n",
    "\n",
    "\n",
    "        print(vote_counts)\n",
    "        if vote_counts[0][1] > n_ballots / 2:\n",
    "            print('Winner in round {}. The winner is {}'.format(n_round, vote_counts[0][0]))\n",
    "            running = False\n",
    "            return vote_counts\n",
    "        else:\n",
    "            \n",
    "            loser = next(x[0] for x in reversed(vote_counts) if x[0] not in losers)\n",
    "            votes[:, loser] = n_candidates + 1\n",
    "            print('No winner in round {}. Loser was {}.'.format(n_round, loser))\n",
    "\n",
    "\n",
    "        if n_round > n_candidates:\n",
    "            print('Err! cancelling')        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from math import log\n",
    "\n",
    "def get_doc_word_votes(doc_ids: List[str], words: list, model: Doc2Vec, coeff=1):\n",
    "    \n",
    "    votes = []\n",
    "    \n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        vec = model.docvecs[doc_id];\n",
    "\n",
    "        sims: list = []\n",
    "\n",
    "        for other_word, cnt in words:\n",
    "            other_vec =  model.wv.get_vector(other_word);\n",
    "            sim = abs(model.wv.cosine_similarities(vec, [other_vec])) * (coeff * cnt)\n",
    "\n",
    "            sims.append(sim) \n",
    "            \n",
    "\n",
    "        \n",
    "        sims_ordered = sorted(sims, reverse=True)\n",
    "        sim_indexes = list([sims_ordered.index(sim) for sim in sims])\n",
    "        votes.append(sim_indexes)\n",
    "        \n",
    "\n",
    "    return np.array(votes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr = np.array(articles)\n",
    "doc_arr_trimmed = doc_arr[:len(t)]\n",
    "docs_in_class = doc_arr_trimmed[t == 3]\n",
    "vecs = list([model.docvecs[doc.id] for doc in docs_in_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(model, doc_ids):\n",
    "    words_union = []\n",
    "    for doc_id in doc_ids:\n",
    "        docvec = model.docvecs[doc_id]\n",
    "        words_union += [wc[0] for wc in model.wv.similar_by_vector(docvec, topn=100, restrict_vocab=10000)]\n",
    "    return [wc for wc in Counter(words_union)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n"
    }
   ],
   "source": [
    "doc_ids = [doc.id for doc in docs_in_class]\n",
    "print(len(docs_in_class))\n",
    "words = get_words(model, doc_ids)\n",
    "words_with_counts = [(word, model.wv.vocab[word].count) for word in words]\n",
    "votes = get_doc_word_votes(doc_ids,words_with_counts, model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['family',\n 'struggle',\n 'pay',\n 'funeral',\n 'face',\n 'rise',\n 'price',\n 'country',\n 'help',\n 'council',\n 'net',\n '£',\n 'surplus',\n 'cremation',\n 'cemetery_burial',\n 'observer',\n 'learn',\n 'surplus',\n 'expect',\n 'rise',\n 'significantly',\n 'year',\n 'result',\n 'death_toll',\n 'figure',\n 'more',\n 'authority',\n 'england',\n 'scotland_wales',\n 'obtain_freedom',\n 'information_request',\n 'see',\n 'observer',\n 'show',\n 'average',\n 'rise',\n 'fee',\n 'year',\n '£',\n 'rate_inflation',\n 'big',\n 'increase',\n 'impose',\n 'trafford',\n 'council',\n 'hike',\n '£',\n '£',\n '£',\n 'birmingham_city',\n 'council',\n 'uk',\n 'big',\n 'local_authority',\n 'make',\n 'large',\n 'surplus',\n 'cremation_burial',\n 'total_£',\n 'm',\n 'charge',\n '£',\n 'cremation',\n 'extra',\n '£',\n 'funeral',\n 'overrun',\n 'time',\n 'worthe',\n 'most_expensive',\n 'council',\n 'provide',\n 'datum',\n 'cremation',\n 'charge',\n '£',\n 'increase',\n '£',\n 'low',\n 'charge',\n '£',\n 'south_west',\n 'middlesex',\n 'council',\n 'say',\n 'surplus',\n 'help',\n 'recoup_cost',\n 'invest',\n 'equipment',\n 'upgrade',\n 'meet',\n 'new',\n 'environmental_standard',\n 'spokesperson',\n 'birmingham_city',\n 'council',\n 'say',\n 'operation',\n 'crematorium',\n 'very',\n 'expensive',\n 'business',\n 'investment',\n 'require',\n 'new',\n 'cremator',\n 'require',\n 'pollution',\n 'control',\n 'technology',\n 'can',\n 'cost',\n 'excess_£',\n 'm',\n 'addition',\n 'routine_maintenance',\n 'see',\n 'impact',\n 'funeral',\n 'poverty',\n 'day',\n 'rob',\n 'people',\n 'mental',\n 'space',\n 'grieve',\n 'love_one',\n 'lindesay',\n 'mace',\n 'campaigner',\n 'spokesman',\n 'worthe',\n 'council',\n 'say',\n 'other',\n 'local_authority',\n 'provide',\n 'figure',\n 'charge',\n 'high',\n 'fee',\n 'invest_heavily',\n 'exceptional',\n 'facility',\n 'beautiful',\n 'environment',\n 'work',\n 'hard',\n 'family',\n 'provide',\n 'sensitive',\n 'comforting',\n 'experience',\n 'proud',\n 'service',\n 'provide',\n 'almost_quarter',\n 'local_authority',\n 'provide',\n 'datum',\n 'offer',\n 'direct_cremation',\n 'body',\n 'cremate',\n 'ceremony',\n 'mourner',\n 'allow',\n 'family',\n 'hold',\n 'memorial_service',\n 'time',\n 'place',\n 'own_choosing',\n 'offer',\n 'direct_cremation',\n 'slot',\n 'advertise',\n 'direct_cremation',\n 'cheap',\n 'average',\n '£',\n 'year',\n 'compare',\n '£',\n 'traditional',\n 'cremation',\n 'funeral',\n 'poverty',\n 'campaigner',\n 'say',\n 'direct_cremation',\n 'effective_way',\n 'cut',\n 'cost',\n 'low_income',\n 'family',\n 'earth',\n 'quaker',\n 'social',\n 'action',\n 'organisation',\n 'call',\n 'cap',\n 'cost',\n 'funeral',\n 'say',\n 'nearly',\n 'people',\n 'struggle',\n 'pay',\n 'funeral',\n 'last_year',\n 'lindesay',\n 'mace',\n 'acting',\n 'manager',\n 'say',\n 'see',\n 'impact',\n 'funeral',\n 'poverty',\n 'day',\n 'push',\n 'people',\n 'debt',\n 'rob',\n 'mental',\n 'space',\n 'grieve',\n 'love_one',\n 'competition_market',\n 'authority',\n 'review',\n 'cremation',\n 'fee',\n 'part',\n 'investigation',\n 'funeral',\n 'industry',\n 'could',\n 'recommend',\n 'price_cap',\n 'stop',\n 'above_inflation',\n 'rise',\n 'provisional_finding',\n 'due',\n 'publish',\n 'summer',\n 'steven',\n 'cains',\n 'founder',\n 'harbour',\n 'funeral',\n 'company',\n 'organise',\n 'direct_cremation',\n 'obtain',\n 'datum',\n 'say',\n 'cost',\n 'cremation',\n 'rise',\n 'far',\n 'inflation',\n 'give',\n 'lack',\n 'competition',\n 'barrier_entry',\n 'new',\n 'crematoria',\n 'more',\n 'people',\n 'choose',\n 'direct_cremation',\n 'direct_cremation',\n 'account',\n 'funeral',\n 'expect',\n 'figure',\n 'continue',\n 'rise',\n 'more',\n 'people',\n 'become_disillusioned',\n 'expensive',\n 'hurried',\n 'traditional',\n 'funeral',\n 'service',\n 'say',\n 'high',\n 'know',\n 'council',\n 'cremation',\n 'charge',\n 'worthe',\n '£',\n 'milton_keynes',\n '£',\n 'peterborough',\n '£',\n 'wakefield',\n 'pontefract',\n '£',\n 'bath',\n '£',\n 'inverness',\n '£',\n 'cheltenham',\n '£',\n 'plymouth',\n '£',\n 'dudley',\n '£',\n 'leed',\n '£']"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "docs_in_class[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 9)]\nWinner in round 1. The winner is 0\n"
    }
   ],
   "source": [
    "winner = av_run(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "average_price\n"
    }
   ],
   "source": [
    "for ballot in winner:\n",
    "    print(words[ballot[0]]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr = np.array(articles)\n",
    "doc_arr_trimmed = doc_arr[:len(t)]\n",
    "docs_in_class = doc_arr_trimmed[t == 2]\n",
    "\n",
    "words = []\n",
    "for doc in docs_in_class:\n",
    "    words += list(np.unique(doc.words))\n",
    "top_words = Counter(words).most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['society/2020/jul/25/grieving-families-pushed-into-debt-as-costs-soar-for-burials-and-cremations',\n 'business/2020/may/11/mourners-choosing-simplified-funerals-during-covid-19-crisis',\n 'business/2020/apr/24/funeral-homes-push-for-state-help-as-lockdown-leads-to-no-frills-services',\n 'business/2020/mar/11/dignity-delays-low-cost-funeral-plan-until-after-competition-report',\n 'society/2020/jan/26/church-of-england-could-seek-end-paupers-funerals',\n 'society/2020/jan/06/cost-of-dying-at-record-high-as-price-of-uk-funeral-exceeds-4400',\n 'australia-news/2019/aug/01/funeral-homes-investigation-reveals-high-prices-and-unexplained-charges',\n 'business/2019/may/13/funeral-provider-dignity-warns-fall-in-number-of-deaths-will-hit-profits',\n 'business/2019/mar/28/competition-watchdog-to-investigate-funeral-sector-as-prices-escalate-cma']"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6347216"
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "model.wv.similarity('magnus_carlsen', 'chess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.theme_extractor.article_preprocess_job import ArticlePreprocessJob\n",
    "\n",
    "apj = ArticlePreprocessJob()\n",
    "\n",
    "raw_articles = apj.get_articles_for_latest_load()[:10000]\n",
    "\n",
    "processed_articles = apj.preprocess_raw_articles(raw_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "tokenized_texts = apj.preprocessor.preprocessed_docs\n",
    "\n",
    "\n",
    "def get_ngrams(tokenized_texts: List[List[str]], phrases: Phrases):\n",
    "    return np.unique([token for token in sum([phrases[phrases[tokens]] for tokens in tokenized_texts ], []) if '_' in token])\n",
    "\n",
    "def comp_phrasers():\n",
    "    \n",
    "    phrases1 = Phrases(tokenized_texts)\n",
    "\n",
    "    phrases2 = Phrases(tokenized_texts, scoring='npmi', threshold=0.5, min_count=50)\n",
    "        \n",
    "    tokens1 = get_ngrams(tokenized_texts, phrases1)\n",
    "    tokens2 = get_ngrams(tokenized_texts, phrases2)\n",
    "\n",
    "    \n",
    "    in1butnot2 = np.setdiff1d(tokens1, tokens2, assume_unique=True)\n",
    "    in2butnot1 = np.setdiff1d(tokens2, tokens1, assume_unique=True)\n",
    "\n",
    "    return tokens1, tokens2, in1butnot2, in2butnot1\n",
    "\n",
    "tokens1, tokens2, unique_to_1, unique_to_2 = comp_phrasers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2030\nanti_-\nas_well\nchief_executive\nclimate_change\ncomic_strip\ndonald_trump\nexecutive_order\nhuman_right\nlast_week\nlast_year\nmajority_country\nmuslim_majority\nnew_york\nnon_-\nprime_minister\nsupreme_court\ntell_guardian\ntheresa_may\ntravel_ban\nunited_states\nwhite_house\n£_m\n"
    }
   ],
   "source": [
    "print(len(unique_to_1))\n",
    "for token in tokens2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['ner', 'parser'])\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_html(res: str) -> str:\n",
    "    soup = BeautifulSoup(res, features=\"lxml\")\n",
    "    \n",
    "    for f in soup.find_all('figure'):\n",
    "        f.decompose()\n",
    "    \n",
    "    text = soup.get_text().lower();\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = extract_text_from_html(raw_articles[0].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['scotland',\n 'yard',\n 'investigate',\n 'claim',\n 'worker',\n 'outsource',\n 'firm',\n 'caput',\n 'pay',\n 'convict',\n 'deliberately',\n 'fit',\n 'electronic',\n 'ankle',\n 'tag',\n 'loosely',\n 'allow',\n 'slip',\n 'device',\n 'when',\n 'want',\n 'go',\n 'staff',\n 'company',\n 'run',\n 'government',\n 'electronic',\n 'monitoring',\n 'service',\n 'allegedly',\n 'pay',\n 'time',\n 'help',\n 'at',\n 'least',\n 'offender',\n 'beat',\n 'court',\n 'impose',\n 'curfew',\n 'accord',\n 'report',\n 'sun',\n 'metropolitan',\n 'police',\n 'say',\n 'investigation',\n 'centre',\n 'london',\n 'borough',\n 'newham',\n 'say',\n 'people',\n 'include',\n 'current',\n 'former',\n 'ems',\n 'worker',\n 'arrest',\n 'connection',\n 'offence',\n 'involve',\n 'monitoring',\n 'offender',\n 'accord',\n 'sun',\n 'scheme',\n 'reveal',\n 'offender',\n 'arrest',\n 'suspicion',\n 'attempt',\n 'murder',\n 'suppose',\n 'home',\n 'curfew',\n 'electronic',\n 'tag',\n 'use',\n 'monitor',\n 'condition',\n 'court',\n 'prison',\n 'order',\n 'usually',\n 'securely',\n 'attach',\n 'ankle',\n 'defender',\n 'can',\n 'remove',\n 'then',\n 'send',\n 'location',\n 'datum',\n 'base',\n 'unit',\n 'offender',\n 'home',\n 'ensure',\n 'remain',\n 'present',\n 'curfew',\n 'hour',\n 'leave',\n 'area',\n 'base',\n 'unit',\n 'send',\n 'alert',\n 'monitor',\n 'centre',\n 'police',\n 'make',\n 'first',\n 'arrest',\n 'case',\n 'january',\n 'when',\n 'hold',\n 'old',\n 'former',\n 'ems',\n 'employee',\n 'romford',\n 'essex',\n 'conspiracy',\n 'pervert',\n 'course',\n 'justice',\n 'theft',\n 'tag',\n 'equipment',\n 'take',\n 'east',\n 'london',\n 'police',\n 'station',\n 'subsequently',\n 'bail',\n 'return',\n 'police',\n 'station',\n 'date',\n 'early',\n 'april',\n 'current',\n 'ems',\n 'worker',\n 'old',\n 'man',\n 'old',\n 'woman',\n 'arrest',\n 'january',\n 'conspiracy',\n 'pervert',\n 'course',\n 'justice',\n 'also',\n 'bail',\n 'return',\n 'date',\n 'early',\n 'april',\n 'further',\n 'people',\n 'none',\n 'employee',\n 'former',\n 'employee',\n 'ems',\n 'arrest',\n 'january',\n 'suspicion',\n 'same',\n 'offence',\n 'capita',\n 'hand',\n 'year',\n 'contract',\n 'electronic',\n 'tagging',\n 'july',\n 'company',\n 'win',\n 'work',\n 'rival',\n 'outsource',\n 'security',\n 'firm',\n 'g4',\n 'serco',\n 'embroil',\n 'overcharge',\n 'allegation',\n 'lead',\n 'repay',\n 'government',\n 'nearly']"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']\n",
    "vals = [token.lemma_ for token in nlp(text) if token.pos_ in allowed_postags and len(token.lemma_) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}