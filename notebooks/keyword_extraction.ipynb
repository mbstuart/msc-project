{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.5 64-bit ('base': conda)",
   "display_name": "Python 3.7.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "00c13f55ba049f4e591e16c55112044268b965477979f20629110d152507d31b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "with path.Path('..'):\n",
    "    from services.theme_extractor.preprocessing import ArticlePreprocessJob, ArticlePreprocessor \n",
    "\n",
    "    from services.theme_extractor.wv_model import  WVModelBuilder, WVModelJob\n",
    "\n",
    "    from services.theme_extractor.clustering import Clusterer, ClusterJob\n",
    "\n",
    "    from services.theme_extractor.keyword_extraction import KeywordExtractor\n",
    "\n",
    "    from services.libs.data_model import ProcessedArticle, Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvm = WVModelJob()\n",
    "\n",
    "al = wvm.get_latest_article_load()\n",
    "\n",
    "model = wvm.get_model_from_disk(al.id)\n",
    "\n",
    "cj = ClusterJob(model, al.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = cj.filter_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = Clusterer(model, articles, al.id, from_scratch=True, min_cluster_size=3, cluster_selection_epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = c.create_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "\n",
    "articles_in_class = np.array(articles)[:len(mapping)][mapping == 1]\n",
    "\n",
    "\n",
    "titles = [art.title_words for art in articles_in_class]\n",
    "\n",
    "vecs = list([model.docvecs[doc.id] for doc in articles_in_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def test_word_extraction_method(docs, method):\n",
    "\n",
    "    keywords = method(docs);\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from typing import List\n",
    "def generate_cngrams(words_list: List[str], n: int):\n",
    "    ngrams_list = []\n",
    " \n",
    "    for num in range(0, len(words_list) - (n - 1)):\n",
    "        ngram = (words_list[num:num + n])\n",
    "        ngrams_list.append(ngram)\n",
    " \n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import statistics as s\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def extract_ngram_keywords(articles_in_class):\n",
    "    n_grams = []\n",
    "\n",
    "    for i, art in enumerate(articles_in_class):\n",
    "        words =  art.title_words if len(articles_in_class) > 5 else art.words;\n",
    "        # n_grams += [[word] for word in words]\n",
    "        n_grams += generate_cngrams(words, 2)\n",
    "        n_grams += generate_cngrams(words, 3)\n",
    "        n_grams += generate_cngrams(words, 4)\n",
    "\n",
    "\n",
    "    print('n_grams obtained - there are {}'.format(len(n_grams)))\n",
    "\n",
    "    n_grams.sort()\n",
    "    n_grams = list(n_grams for n_grams,_ in itertools.groupby(n_grams))\n",
    "\n",
    "    print('dupes removed - there are now {}'.format(len(n_grams)))\n",
    "\n",
    "    scores = {}\n",
    "    docvecs = np.array(vecs)\n",
    "\n",
    "    p_vecs = [model.infer_vector(n_gram, steps=10) for n_gram in n_grams]\n",
    "    p_vecs_arr = np.array(p_vecs).reshape(len(p_vecs), 400)\n",
    "    sims = np.mean(cosine_similarity(p_vecs_arr, docvecs), axis=1)\n",
    "\n",
    "    scores = dict(enumerate(sims))\n",
    "    \n",
    "    def convert_ngram_to_string(ngram: List[str]):\n",
    "        return \" \".join(ngram).replace(\"_\", \" \")\n",
    "\n",
    "    return [convert_ngram_to_string(n_grams[p[0]]) for p in Counter(scores).most_common(10)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.9047619 , 0.80952381])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [1 ,2 ,3],\n",
    "    [3, 2, 1]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [1 ,2 ,3],\n",
    "    [3, 2, 1],\n",
    "    [2, 4, 6]\n",
    "])\n",
    "\n",
    "sim = cosine_similarity(a,b)\n",
    "\n",
    "np.mean(sim, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(articles_in_class):\n",
    "\n",
    "    words = sum([art.words for art in articles_in_class], [])\n",
    "\n",
    "    return [w[0] for w in Counter(words).most_common(10)]\n",
    "\n",
    "def get_most_common_title_words(articles_in_class):\n",
    "\n",
    "    words = sum([art.title_words for art in articles_in_class], [])\n",
    "\n",
    "    return [w[0] for w in Counter(words).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "def textrank_vanilla_blob(articles_in_class):\n",
    "    words = []\n",
    "    for article in articles_in_class:\n",
    "        words += article.words\n",
    "        \n",
    "        \n",
    "    \n",
    "    return keywords(\" \".join(words), split=True)[:10]\n",
    "\n",
    "def textrank_vanilla(articles_in_class):\n",
    "    kwords = []\n",
    "    for article in articles_in_class:\n",
    "        kwords += keywords(\" \".join(article.words), split=True)\n",
    "    \n",
    "    return [w[0] for w in Counter(kwords).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "n_grams obtained - there are 807\ndupes removed - there are now 757\nn_grams obtained - there are 138\ndupes removed - there are now 132\nn_grams obtained - there are 3591\ndupes removed - there are now 3484\nn_grams obtained - there are 114\ndupes removed - there are now 108\nn_grams obtained - there are 2682\ndupes removed - there are now 2394\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Andy Ruiz Jr rematch</td>\n      <td>english language test</td>\n      <td>employ company</td>\n      <td>moratorium end</td>\n      <td>testing monitoring</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WBC heavyweight title</td>\n      <td>student accuse cheat</td>\n      <td>defra spokeswoman say</td>\n      <td>water bill rise</td>\n      <td>monitoring social distancing strict quarantine</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>boxer Maxim Dadashev</td>\n      <td>MPs hold inquiry</td>\n      <td>represent more half council england wales</td>\n      <td>clean water shutoff</td>\n      <td>check temperature</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Joshua Ruiz Jr rematch</td>\n      <td>english test cheat</td>\n      <td>fly tipping penalty increase</td>\n      <td>layoff trigger</td>\n      <td>medical worker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>boxer Maxim</td>\n      <td>act immigration</td>\n      <td>resource waste</td>\n      <td>million Americans</td>\n      <td>majority national assembly</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fight Nurmagomedov Diaz Mayweather</td>\n      <td>hold inquiry</td>\n      <td>harm pet wildlife</td>\n      <td>million US</td>\n      <td>worry so be sure</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Logan Paul boxing</td>\n      <td>face legal action</td>\n      <td>waste clearance</td>\n      <td>water shutoff</td>\n      <td>fall mortality rate</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fight Kubrat Pulev</td>\n      <td>english test</td>\n      <td>local government association say council</td>\n      <td>trigger pandemic</td>\n      <td>accord korea</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rematch confirm</td>\n      <td>should replace</td>\n      <td>tough guideline issue</td>\n      <td>bill rise</td>\n      <td>monitoring social distancing strict</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fight Nurmagomedov Diaz</td>\n      <td>english test cheat claim</td>\n      <td>effective punitive deterrent</td>\n      <td>shutoff layoff trigger</td>\n      <td>win seat assembly</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fight</td>\n      <td>home_office</td>\n      <td>council</td>\n      <td>water</td>\n      <td>south_korea</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>say</td>\n      <td>student</td>\n      <td>fine</td>\n      <td>bill</td>\n      <td>party</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joshua</td>\n      <td>test</td>\n      <td>issue</td>\n      <td>shutoff</td>\n      <td>say</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>will</td>\n      <td>say</td>\n      <td>litter</td>\n      <td>city</td>\n      <td>election</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>go</td>\n      <td>cheat</td>\n      <td>say</td>\n      <td>say</td>\n      <td>moon</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fury</td>\n      <td>uk</td>\n      <td>more</td>\n      <td>detroit</td>\n      <td>outbreak</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>when</td>\n      <td>english</td>\n      <td>also</td>\n      <td>household</td>\n      <td>polling_station</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>would</td>\n      <td>people</td>\n      <td>fly_tipping</td>\n      <td>resident</td>\n      <td>hold</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fighter</td>\n      <td>study</td>\n      <td>fly</td>\n      <td>people</td>\n      <td>vote</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ruiz</td>\n      <td>government</td>\n      <td>increase</td>\n      <td>running_water</td>\n      <td>voter</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Anthony_Joshua</td>\n      <td>test</td>\n      <td>council</td>\n      <td>water</td>\n      <td>South_Korea</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ruiz</td>\n      <td>Home_Office</td>\n      <td>litter</td>\n      <td>shutoff</td>\n      <td>rule</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Andy</td>\n      <td>English</td>\n      <td>unpunishe</td>\n      <td>million</td>\n      <td>party</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jr</td>\n      <td>scandal</td>\n      <td>many</td>\n      <td>US</td>\n      <td>win_election</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fight</td>\n      <td>cheat</td>\n      <td>England_Wales</td>\n      <td>Detroit</td>\n      <td>landslide</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Tyson</td>\n      <td>student</td>\n      <td>tough_penalty</td>\n      <td>pandemic</td>\n      <td>coronavirus_outbreak</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Fury</td>\n      <td>MPs</td>\n      <td>need</td>\n      <td>suspend</td>\n      <td>south_korean</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>rematch</td>\n      <td>UK</td>\n      <td>curb</td>\n      <td>face</td>\n      <td>voter</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Eddie</td>\n      <td>face</td>\n      <td>surge</td>\n      <td>lose</td>\n      <td>expect</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hearn</td>\n      <td>rush</td>\n      <td>fly</td>\n      <td>water_supply</td>\n      <td>return</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fight</td>\n      <td>student</td>\n      <td>government</td>\n      <td>people</td>\n      <td>moon</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>boxing</td>\n      <td>testing</td>\n      <td>average</td>\n      <td>pandemic</td>\n      <td>coronavirus</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joshua</td>\n      <td>people</td>\n      <td>councillor</td>\n      <td>month</td>\n      <td>glove</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>time</td>\n      <td>visa</td>\n      <td>use</td>\n      <td>running_water</td>\n      <td>vote</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>want</td>\n      <td>uk_government</td>\n      <td>council</td>\n      <td>family</td>\n      <td>party</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ruiz</td>\n      <td>year</td>\n      <td>year</td>\n      <td>department</td>\n      <td>south_korea</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>round</td>\n      <td>fraud</td>\n      <td>sentence</td>\n      <td>water</td>\n      <td>elect</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>box</td>\n      <td>accusation</td>\n      <td>need</td>\n      <td>more_people</td>\n      <td>hold</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>look</td>\n      <td>questionable</td>\n      <td>fine week litterer</td>\n      <td>utility</td>\n      <td>country</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>know</td>\n      <td>cheating</td>\n      <td>council issue</td>\n      <td>work</td>\n      <td>wear_mask</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fight_back</td>\n      <td>home_office</td>\n      <td>council issue</td>\n      <td>water</td>\n      <td>voting</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fighting</td>\n      <td>student</td>\n      <td>littering</td>\n      <td>city</td>\n      <td>south_korea rule party</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fights</td>\n      <td>cheat</td>\n      <td>fine week litterer</td>\n      <td>people</td>\n      <td>voter</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>confirm fight world</td>\n      <td>government</td>\n      <td>last_year</td>\n      <td>many_people</td>\n      <td>vote about_people</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ruiz_jr</td>\n      <td>uk_government</td>\n      <td>relate litter</td>\n      <td>least_people</td>\n      <td>elect</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>box right_now</td>\n      <td>organise cheating</td>\n      <td>fly_tipping</td>\n      <td>pandemic</td>\n      <td>coronavirus</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>time</td>\n      <td>last_year</td>\n      <td>year enforcement</td>\n      <td>detroiter result</td>\n      <td>polling_station</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>time_when</td>\n      <td>almost_year</td>\n      <td>increase</td>\n      <td>resident</td>\n      <td>national_assembly election</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>times</td>\n      <td>people</td>\n      <td>increase_amount</td>\n      <td>pay</td>\n      <td>country</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>timing</td>\n      <td>hundred_people</td>\n      <td>more_people</td>\n      <td>afford</td>\n      <td>other_country</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "def get_performance_for_keyword_extraction(name, keyword_extraction_method):\n",
    "\n",
    "    descs = [\n",
    "        'Boxing News',\n",
    "        'Home Office English Tests Scandal',\n",
    "        'Littering in the UK',\n",
    "        'US Water Shutoffs',\n",
    "        'South Korea Election 2020'\n",
    "    ]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, theme_id in enumerate([1, 10, 20, 64, 95]):\n",
    "        articles_in_class = np.array(articles)[:len(mapping)][mapping == theme_id]\n",
    "        desc = descs[i]\n",
    "        data[desc] = keyword_extraction_method(articles_in_class)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "methods = [\n",
    "    ('ngram method', extract_ngram_keywords),\n",
    "    ('common words', get_most_common_words),\n",
    "    ('common words (from title)', get_most_common_title_words),\n",
    "    ('textrank + vote', textrank_vanilla),\n",
    "    ('textrank over all articles', textrank_vanilla_blob)\n",
    "]\n",
    "\n",
    "for name, meth in methods:\n",
    "    get_performance_for_keyword_extraction(name, meth);\n",
    "        \n",
    "        # print([art.title for art in articles_in_class][:10])"
   ]
  }
 ]
}