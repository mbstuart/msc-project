{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599899202263",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import services.theme_extractor_api.main\n",
    "from services.theme_extractor.cluster_job import ClusterJob\n",
    "from services.theme_extractor.wv_model_job import WVModelJob\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvm = WVModelJob()\n",
    "\n",
    "al = wvm.get_latest_article_load()\n",
    "\n",
    "model = wvm.get_model_from_disk(al.id)\n",
    "\n",
    "cj = ClusterJob(model, al.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = cj.filter_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.theme_extractor.clusterer import Clusterer\n",
    "\n",
    "\n",
    "c = Clusterer(model, articles, al.id, from_scratch=True, min_cluster_size=3, cluster_selection_epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = c.create_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "\n",
    "articles_in_class = np.array(articles)[:len(mapping)][mapping == 1]\n",
    "\n",
    "\n",
    "titles = [art.title_words for art in articles_in_class]\n",
    "\n",
    "vecs = list([model.docvecs[doc.id] for doc in articles_in_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def test_word_extraction_method(docs, method):\n",
    "\n",
    "    keywords = method(docs);\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from typing import List\n",
    "def generate_cngrams(words_list: List[str], n: int):\n",
    "    ngrams_list = []\n",
    " \n",
    "    for num in range(0, len(words_list) - (n - 1)):\n",
    "        ngram = (words_list[num:num + n])\n",
    "        ngrams_list.append(ngram)\n",
    " \n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import statistics as s\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def extract_ngram_keywords(articles_in_class):\n",
    "    n_grams = []\n",
    "\n",
    "    for i, art in enumerate(articles_in_class):\n",
    "        words =  art.title_words if len(articles_in_class) > 5 else art.words;\n",
    "        # n_grams += [[word] for word in words]\n",
    "        n_grams += generate_cngrams(words, 2)\n",
    "        n_grams += generate_cngrams(words, 3)\n",
    "        n_grams += generate_cngrams(words, 4)\n",
    "\n",
    "\n",
    "    print('n_grams obtained - there are {}'.format(len(n_grams)))\n",
    "\n",
    "    n_grams.sort()\n",
    "    n_grams = list(n_grams for n_grams,_ in itertools.groupby(n_grams))\n",
    "\n",
    "    print('dupes removed - there are now {}'.format(len(n_grams)))\n",
    "\n",
    "    scores = {}\n",
    "    docvecs = np.array(vecs)\n",
    "\n",
    "    p_vecs = [model.infer_vector(n_gram, steps=10) for n_gram in n_grams]\n",
    "    p_vecs_arr = np.array(p_vecs).reshape(len(p_vecs), 400)\n",
    "    sims = np.mean(cosine_similarity(p_vecs_arr, docvecs), axis=1)\n",
    "\n",
    "    scores = dict(enumerate(sims))\n",
    "    \n",
    "    def convert_ngram_to_string(ngram: List[str]):\n",
    "        return \" \".join(ngram).replace(\"_\", \" \")\n",
    "\n",
    "    return [convert_ngram_to_string(n_grams[p[0]]) for p in Counter(scores).most_common(10)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.9047619 , 0.80952381])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [1 ,2 ,3],\n",
    "    [3, 2, 1]\n",
    "])\n",
    "\n",
    "b = np.array([\n",
    "    [1 ,2 ,3],\n",
    "    [3, 2, 1],\n",
    "    [2, 4, 6]\n",
    "])\n",
    "\n",
    "sim = cosine_similarity(a,b)\n",
    "\n",
    "np.mean(sim, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(articles_in_class):\n",
    "\n",
    "    words = sum([art.words for art in articles_in_class], [])\n",
    "\n",
    "    return [w[0] for w in Counter(words).most_common(10)]\n",
    "\n",
    "def get_most_common_title_words(articles_in_class):\n",
    "\n",
    "    words = sum([art.title_words for art in articles_in_class], [])\n",
    "\n",
    "    return [w[0] for w in Counter(words).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "def textrank_vanilla_blob(articles_in_class):\n",
    "    words = []\n",
    "    for article in articles_in_class:\n",
    "        words += article.words\n",
    "        \n",
    "        \n",
    "    \n",
    "    return keywords(\" \".join(words), split=True)[:10]\n",
    "\n",
    "def textrank_vanilla(articles_in_class):\n",
    "    kwords = []\n",
    "    for article in articles_in_class:\n",
    "        kwords += keywords(\" \".join(article.words), split=True)\n",
    "    \n",
    "    return [w[0] for w in Counter(kwords).most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "n_grams obtained - there are 807\ndupes removed - there are now 757\nn_grams obtained - there are 138\ndupes removed - there are now 132\nn_grams obtained - there are 3591\ndupes removed - there are now 3484\nn_grams obtained - there are 114\ndupes removed - there are now 108\nn_grams obtained - there are 2682\ndupes removed - there are now 2394\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Andy Ruiz Jr rematch</td>\n      <td>english language test</td>\n      <td>employ company</td>\n      <td>moratorium end</td>\n      <td>testing monitoring</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WBC heavyweight title</td>\n      <td>student accuse cheat</td>\n      <td>defra spokeswoman say</td>\n      <td>water bill rise</td>\n      <td>monitoring social distancing strict quarantine</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>boxer Maxim Dadashev</td>\n      <td>MPs hold inquiry</td>\n      <td>represent more half council england wales</td>\n      <td>clean water shutoff</td>\n      <td>check temperature</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Joshua Ruiz Jr rematch</td>\n      <td>english test cheat</td>\n      <td>fly tipping penalty increase</td>\n      <td>layoff trigger</td>\n      <td>medical worker</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>boxer Maxim</td>\n      <td>act immigration</td>\n      <td>resource waste</td>\n      <td>million Americans</td>\n      <td>majority national assembly</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fight Nurmagomedov Diaz Mayweather</td>\n      <td>hold inquiry</td>\n      <td>harm pet wildlife</td>\n      <td>million US</td>\n      <td>worry so be sure</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Logan Paul boxing</td>\n      <td>face legal action</td>\n      <td>waste clearance</td>\n      <td>water shutoff</td>\n      <td>fall mortality rate</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>fight Kubrat Pulev</td>\n      <td>english test</td>\n      <td>local government association say council</td>\n      <td>trigger pandemic</td>\n      <td>accord korea</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rematch confirm</td>\n      <td>should replace</td>\n      <td>tough guideline issue</td>\n      <td>bill rise</td>\n      <td>monitoring social distancing strict</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fight Nurmagomedov Diaz</td>\n      <td>english test cheat claim</td>\n      <td>effective punitive deterrent</td>\n      <td>shutoff layoff trigger</td>\n      <td>win seat assembly</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fight</td>\n      <td>home_office</td>\n      <td>council</td>\n      <td>water</td>\n      <td>south_korea</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>say</td>\n      <td>student</td>\n      <td>fine</td>\n      <td>bill</td>\n      <td>party</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joshua</td>\n      <td>test</td>\n      <td>issue</td>\n      <td>shutoff</td>\n      <td>say</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>will</td>\n      <td>say</td>\n      <td>litter</td>\n      <td>city</td>\n      <td>election</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>go</td>\n      <td>cheat</td>\n      <td>say</td>\n      <td>say</td>\n      <td>moon</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fury</td>\n      <td>uk</td>\n      <td>more</td>\n      <td>detroit</td>\n      <td>outbreak</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>when</td>\n      <td>english</td>\n      <td>also</td>\n      <td>household</td>\n      <td>polling_station</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>would</td>\n      <td>people</td>\n      <td>fly_tipping</td>\n      <td>resident</td>\n      <td>hold</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>fighter</td>\n      <td>study</td>\n      <td>fly</td>\n      <td>people</td>\n      <td>vote</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ruiz</td>\n      <td>government</td>\n      <td>increase</td>\n      <td>running_water</td>\n      <td>voter</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Anthony_Joshua</td>\n      <td>test</td>\n      <td>council</td>\n      <td>water</td>\n      <td>South_Korea</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ruiz</td>\n      <td>Home_Office</td>\n      <td>litter</td>\n      <td>shutoff</td>\n      <td>rule</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Andy</td>\n      <td>English</td>\n      <td>unpunishe</td>\n      <td>million</td>\n      <td>party</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jr</td>\n      <td>scandal</td>\n      <td>many</td>\n      <td>US</td>\n      <td>win_election</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fight</td>\n      <td>cheat</td>\n      <td>England_Wales</td>\n      <td>Detroit</td>\n      <td>landslide</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Tyson</td>\n      <td>student</td>\n      <td>tough_penalty</td>\n      <td>pandemic</td>\n      <td>coronavirus_outbreak</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Fury</td>\n      <td>MPs</td>\n      <td>need</td>\n      <td>suspend</td>\n      <td>south_korean</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>rematch</td>\n      <td>UK</td>\n      <td>curb</td>\n      <td>face</td>\n      <td>voter</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Eddie</td>\n      <td>face</td>\n      <td>surge</td>\n      <td>lose</td>\n      <td>expect</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hearn</td>\n      <td>rush</td>\n      <td>fly</td>\n      <td>water_supply</td>\n      <td>return</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fight</td>\n      <td>student</td>\n      <td>government</td>\n      <td>people</td>\n      <td>moon</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>boxing</td>\n      <td>testing</td>\n      <td>average</td>\n      <td>pandemic</td>\n      <td>coronavirus</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>joshua</td>\n      <td>people</td>\n      <td>councillor</td>\n      <td>month</td>\n      <td>glove</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>time</td>\n      <td>visa</td>\n      <td>use</td>\n      <td>running_water</td>\n      <td>vote</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>want</td>\n      <td>uk_government</td>\n      <td>council</td>\n      <td>family</td>\n      <td>party</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ruiz</td>\n      <td>year</td>\n      <td>year</td>\n      <td>department</td>\n      <td>south_korea</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>round</td>\n      <td>fraud</td>\n      <td>sentence</td>\n      <td>water</td>\n      <td>elect</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>box</td>\n      <td>accusation</td>\n      <td>need</td>\n      <td>more_people</td>\n      <td>hold</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>look</td>\n      <td>questionable</td>\n      <td>fine week litterer</td>\n      <td>utility</td>\n      <td>country</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>know</td>\n      <td>cheating</td>\n      <td>council issue</td>\n      <td>work</td>\n      <td>wear_mask</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Boxing News</th>\n      <th>Home Office English Tests Scandal</th>\n      <th>Littering in the UK</th>\n      <th>US Water Shutoffs</th>\n      <th>South Korea Election 2020</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fight_back</td>\n      <td>home_office</td>\n      <td>council issue</td>\n      <td>water</td>\n      <td>voting</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fighting</td>\n      <td>student</td>\n      <td>littering</td>\n      <td>city</td>\n      <td>south_korea rule party</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fights</td>\n      <td>cheat</td>\n      <td>fine week litterer</td>\n      <td>people</td>\n      <td>voter</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>confirm fight world</td>\n      <td>government</td>\n      <td>last_year</td>\n      <td>many_people</td>\n      <td>vote about_people</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ruiz_jr</td>\n      <td>uk_government</td>\n      <td>relate litter</td>\n      <td>least_people</td>\n      <td>elect</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>box right_now</td>\n      <td>organise cheating</td>\n      <td>fly_tipping</td>\n      <td>pandemic</td>\n      <td>coronavirus</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>time</td>\n      <td>last_year</td>\n      <td>year enforcement</td>\n      <td>detroiter result</td>\n      <td>polling_station</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>time_when</td>\n      <td>almost_year</td>\n      <td>increase</td>\n      <td>resident</td>\n      <td>national_assembly election</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>times</td>\n      <td>people</td>\n      <td>increase_amount</td>\n      <td>pay</td>\n      <td>country</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>timing</td>\n      <td>hundred_people</td>\n      <td>more_people</td>\n      <td>afford</td>\n      <td>other_country</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "def get_performance_for_keyword_extraction(name, keyword_extraction_method):\n",
    "\n",
    "    descs = [\n",
    "        'Boxing News',\n",
    "        'Home Office English Tests Scandal',\n",
    "        'Littering in the UK',\n",
    "        'US Water Shutoffs',\n",
    "        'South Korea Election 2020'\n",
    "    ]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for i, theme_id in enumerate([1, 10, 20, 64, 95]):\n",
    "        articles_in_class = np.array(articles)[:len(mapping)][mapping == theme_id]\n",
    "        desc = descs[i]\n",
    "        data[desc] = keyword_extraction_method(articles_in_class)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "methods = [\n",
    "    ('ngram method', extract_ngram_keywords),\n",
    "    ('common words', get_most_common_words),\n",
    "    ('common words (from title)', get_most_common_title_words),\n",
    "    ('textrank + vote', textrank_vanilla),\n",
    "    ('textrank over all articles', textrank_vanilla_blob)\n",
    "]\n",
    "\n",
    "for name, meth in methods:\n",
    "    get_performance_for_keyword_extraction(name, meth);\n",
    "        \n",
    "        # print([art.title for art in articles_in_class][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8000"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "\n",
    "\n",
    "a =  np.ones(20)\n",
    "len(cartesian_product(a, a, a).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "from services.libs.data_model.processed_article import ProcessedArticle\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from  services.theme_extractor.logger import logger\n",
    "\n",
    "\n",
    "\n",
    "from services.libs.data_model.theme import Theme\n",
    "from services.libs.data_model.article import Article\n",
    "        \n",
    "def __get_class_words_from_doc_selection(docs_in_class: List[ProcessedArticle], vecs, model: Doc2Vec):\n",
    "        doc_dict = {}\n",
    "\n",
    "        for doc in docs_in_class:\n",
    "            for word in doc.words:\n",
    "                if word in doc_dict:\n",
    "                    doc_dict[word] += 1\n",
    "                else:\n",
    "                    doc_dict[word] = 1\n",
    "\n",
    "        d = Counter(doc_dict)\n",
    "\n",
    "        top_words = d.most_common(1000)\n",
    "\n",
    "        word_2_vec_ranking = {}\n",
    "\n",
    "        for word in top_words:\n",
    "            \n",
    "            if(word[0] not in model.wv.vocab):\n",
    "                continue;\n",
    "            \n",
    "            word_vec = model[word[0]]\n",
    "            av_vec = np.average(vecs, axis=0)\n",
    "\n",
    "\n",
    "            similarity = 1 - spatial.distance.cosine(word_vec, av_vec)\n",
    "            word_2_vec_ranking[word[0]] = similarity\n",
    "\n",
    "        rank_counter = Counter(word_2_vec_ranking)\n",
    "\n",
    "        return [w[0] for w in rank_counter.most_common(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a9f79b37a2b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdoc_arr_trimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdocs_in_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_arr_trimmed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs_in_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "doc_arr = np.array(articles)\n",
    "doc_arr_trimmed = doc_arr[:len(t)]\n",
    "docs_in_class = doc_arr_trimmed[t == 0]\n",
    "vecs = list([model.docvecs[doc.id] for doc in docs_in_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'chess'"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "title_words = sum([[w.lemma_ for w in nlp(doc.title) if w.pos_ in ['NOUN', 'VERB',  'PROPN']] for doc in docs_in_class], [])\n",
    "top_word = Counter(title_words).most_common(1)[0][0]\n",
    "top_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['levon_aronian' '86']\n ['nf3_nc6' '57']\n ['e4_e5' '61']\n ['wesley_so' '70']\n ['anish_giri' '55']\n ['sergey_karjakin' '105']\n ['teimour' '14']\n ['radjabov' '22']\n ['luke_mcshane' '58']\n ['vishy_anand' '94']\n ['wijk' '73']\n ['bc4' '43']\n ['aronian' '63']\n ['dubov' '41']\n ['mamedyarov' '67']\n ['bc5' '39']\n ['lagrave' '83']\n ['relate_chess' '133']\n ['ding_liren' '66']\n ['lagno' '10']\n ['bb5' '54']\n ['karpov' '48']\n ['d3' '59']\n ['fabiano_caruana' '193']\n ['grenke' '26']\n ['caruana' '321']\n ['shamkir' '33']\n ['nakamura' '99']\n ['firouzja' '89']\n ['garry_kasparov' '113']\n ['maxime_vachi' '69']\n ['nf6' '145']\n ['nf3' '155']\n ['e6' '122']\n ['nc3' '165']\n ['magnus_carlsen' '355']\n ['artemiev' '16']\n ['d4' '218']\n ['hikaru_nakamura' '91']\n ['bd4' '19']\n ['vlad_kramnik' '61']\n ['bb4' '47']\n ['gawain_jones' '83']\n ['c6' '76']\n ['d6' '106']\n ['d5' '192']\n ['shak' '45']\n ['e5' '141']\n ['exd5' '64']\n ['vachier' '14']\n ['nd5' '59']\n ['f4' '98']\n ['bobby_fischer' '79']\n ['e4' '224']\n ['f5' '80']\n ['d2' '16']\n ['c5' '175']\n ['dxe4' '31']\n ['kg7' '40']\n ['grandmaster' '308']\n ['top_seed' '116']\n ['qh8' '15']\n ['goryachkina' '10']\n ['bf4' '46']\n ['h3' '71']\n ['ne7' '38']\n ['h6' '92']\n ['nxe5' '29']\n ['nc6' '90']\n ['yangyi' '17']\n ['qe7' '45']\n ['grischuk' '62']\n ['olympiad' '173']\n ['stavanger' '69']\n ['alekseenko' '7']\n ['re1' '55']\n ['kg2' '33']\n ['h5' '75']\n ['a6' '93']\n ['cxd4' '67']\n ['david_howell' '98']\n ['qd4' '26']\n ['ivanchuk' '16']\n ['qg4' '30']\n ['f6' '53']\n ['ne4' '53']\n ['vachi_lagrave' '29']\n ['qh3' '25']\n ['b3' '70']\n ['be3' '68']\n ['re8' '74']\n ['carlsen' '1009']\n ['be6' '50']\n ['yifan' '48']\n ['nigel_short' '84']\n ['wenjun' '21']\n ['rapid_blitz' '74']\n ['karjakin' '153']\n ['b5' '100']\n ['h4' '71']\n ['hou' '124']\n ['qd7' '31']\n ['qh7' '16']\n ['qh5' '46']\n ['c3' '96']\n ['qf5' '29']\n ['a5' '83']\n ['daniil' '120']\n ['carlsen_world' '15']\n ['nxd4' '67']\n ['classical_game' '61']\n ['vladislav' '36']\n ['altibox' '23']\n ['kh5' '20']\n ['b4' '115']\n ['ding' '182']\n ['g6' '144']\n ['giri' '64']\n ['kasparov' '70']\n ['anatoly' '70']\n ['c4' '219']\n ['praggnanandhaa' '30']\n ['liren' '25']\n ['spassky' '41']\n ['g5' '151']\n ['najdorf' '28']\n ['g3' '117']\n ['gms' '124']\n ['rausis' '17']\n ['michael_adams' '115']\n ['alireza' '51']\n ['outclass' '87']\n ['tiviakov' '23']\n ['sinquefield' '68']\n ['zee' '60']\n ['mikhail' '234']\n ['rook' '195']\n ['koneru' '7']\n ['wijk_aan' '50']\n ['mcshane' '85']\n ['anand' '137']\n ['a4' '123']\n ['svidler' '47']\n ['chess' '1073']\n ['armenia' '211']\n ['bunratty' '10']\n ['grünfeld' '10']\n ['chess.com' '28']\n ['rank_world' '181']\n ['biel' '41']\n ['chess24.com' '24']\n ['tie_break' '339']\n ['polgar' '33']\n ['final_round' '488']\n ['nepomniachtchi' '57']\n ['interzonal' '10']\n ['checkmate' '39']\n ['gelfand' '16']\n ['title_challenger' '29']\n ['world_title' '503']\n ['kazakhstan' '358']\n ['chessable' '12']\n ['endgame' '205']\n ['judit' '31']\n ['opening_round' '168']\n ['ju' '84']\n ['yi' '153']\n ['haria' '16']\n ['fide' '300']\n ['ekaterinburg' '42']\n ['outplay' '116']\n ['qualifier' '751']\n ['wei' '201']\n ['xiong' '76']\n ['a2' '51']\n ['pawn' '533']\n ['zagreb' '133']\n ['cornishman' '33']\n ['lindores' '8']\n ['reykjavik' '99']\n ['astana' '180']\n ['prodigy' '214']\n ['alphazero' '41']\n ['hao' '46']\n ['classical' '596']\n ['first_round' '1669']\n ['chess24' '13']\n ['arkady' '69']\n ['tal' '149']\n ['pairing' '169']\n ['abidjan' '33']\n ['play_off' '565']\n ['main_rival' '212']\n ['seed' '1832']\n ['silver_medal' '128']\n ['point_ahead' '377']\n ['gambit' '149']\n ['azerbaijan' '556']\n ['steinitz' '19']\n ['quartet' '336']]\n"
    }
   ],
   "source": [
    "weighted = np.array([model.wv.get_vector(word) for word in theme_words])\n",
    "counts = np.reshape(np.array([model.wv.vocab[word].count  for word in theme_words]), (200, 1))\n",
    "print(np.array([(word, model.wv.vocab[word].count) for word in theme_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200, 400)\n"
    }
   ],
   "source": [
    "weighted_counts = weighted * counts\n",
    "print(weighted_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('levon_aronian', 'PROPN'),\n ('nf3_nc6', 'PROPN'),\n ('e4_e5', 'PROPN'),\n ('wesley_so', 'ADV'),\n ('anish_giri', 'ADV'),\n ('sergey_karjakin', 'PROPN'),\n ('teimour', 'NOUN'),\n ('radjabov', 'PROPN'),\n ('luke_mcshane', 'PROPN'),\n ('vishy_anand', 'PROPN'),\n ('wijk', 'PROPN'),\n ('bc4', 'PROPN'),\n ('aronian', 'ADJ'),\n ('dubov', 'NOUN'),\n ('mamedyarov', 'PROPN'),\n ('bc5', 'PROPN'),\n ('lagrave', 'NOUN'),\n ('relate_chess', 'PROPN'),\n ('ding_liren', 'PROPN'),\n ('lagno', 'NOUN'),\n ('bb5', 'PROPN'),\n ('karpov', 'PROPN'),\n ('d3', 'PROPN'),\n ('fabiano_caruana', 'PROPN'),\n ('grenke', 'PROPN'),\n ('caruana', 'PROPN'),\n ('shamkir', 'PROPN'),\n ('nakamura', 'PROPN'),\n ('firouzja', 'PROPN'),\n ('garry_kasparov', 'PROPN'),\n ('maxime_vachi', 'PROPN'),\n ('nf6', 'PROPN'),\n ('nf3', 'PROPN'),\n ('e6', 'PROPN'),\n ('nc3', 'PROPN'),\n ('magnus_carlsen', 'PROPN'),\n ('artemiev', 'PROPN'),\n ('d4', 'NOUN'),\n ('hikaru_nakamura', 'ADP'),\n ('bd4', 'PROPN'),\n ('vlad_kramnik', 'X'),\n ('bb4', 'X'),\n ('gawain_jone', 'NOUN'),\n ('c6', 'PROPN'),\n ('d6', 'PROPN'),\n ('d5', 'PROPN'),\n ('shak', 'PROPN'),\n ('e5', 'PROPN'),\n ('exd5', 'PROPN'),\n ('vachier', 'PROPN'),\n ('nd5', 'PROPN'),\n ('f4', 'PROPN'),\n ('bobby_fischer', 'PROPN'),\n ('e4', 'PROPN'),\n ('f5', 'PROPN'),\n ('d2', 'PROPN'),\n ('c5', 'PROPN'),\n ('dxe4', 'PROPN'),\n ('kg7', 'NOUN'),\n ('grandmaster', 'NOUN'),\n ('top_seed', 'PROPN'),\n ('qh8', 'PROPN'),\n ('goryachkina', 'PROPN'),\n ('bf4', 'PROPN'),\n ('h3', 'PROPN'),\n ('ne7', 'PROPN'),\n ('h6', 'PROPN'),\n ('nxe5', 'PROPN'),\n ('nc6', 'PROPN'),\n ('yangyi', 'PROPN'),\n ('qe7', 'PROPN'),\n ('grischuk', 'PROPN'),\n ('olympiad', 'PROPN'),\n ('stavanger', 'PROPN'),\n ('alekseenko', 'PROPN'),\n ('re1', 'PROPN'),\n ('kg2', 'PROPN'),\n ('h5', 'PROPN'),\n ('a6', 'PROPN'),\n ('cxd4', 'PROPN'),\n ('david_howell', 'PUNCT'),\n ('qd4', 'PROPN'),\n ('ivanchuk', 'PROPN'),\n ('qg4', 'PROPN'),\n ('f6', 'PROPN'),\n ('ne4', 'PROPN'),\n ('vachi_lagrave', 'PROPN'),\n ('qh3', 'PROPN'),\n ('b3', 'PROPN'),\n ('be3', 'PROPN'),\n ('re8', 'PROPN'),\n ('carlsen', 'VERB'),\n ('be6', 'PROPN'),\n ('yifan', 'PROPN'),\n ('nigel_short', 'PROPN'),\n ('wenjun', 'PROPN'),\n ('rapid_blitz', 'PROPN'),\n ('karjakin', 'PROPN'),\n ('b5', 'PROPN'),\n ('h4', 'PROPN'),\n ('hou', 'PROPN'),\n ('qd7', 'PROPN'),\n ('qh7', 'PROPN'),\n ('qh5', 'PROPN'),\n ('c3', 'PROPN'),\n ('qf5', 'NOUN'),\n ('a5', 'PROPN'),\n ('daniil', 'NOUN'),\n ('carlsen_world', 'PROPN'),\n ('nxd4', 'PROPN'),\n ('classical_game', 'PROPN'),\n ('vladislav', 'PROPN'),\n ('altibox', 'PROPN'),\n ('kh5', 'PROPN'),\n ('b4', 'PROPN'),\n ('ding', 'PROPN'),\n ('g6', 'PROPN'),\n ('giri', 'PROPN'),\n ('kasparov', 'PROPN'),\n ('anatoly', 'PROPN'),\n ('c4', 'PROPN'),\n ('praggnanandhaa', 'PROPN'),\n ('liren', 'PROPN'),\n ('spassky', 'PROPN'),\n ('g5', 'PROPN'),\n ('najdorf', 'PROPN'),\n ('g3', 'PROPN'),\n ('gms', 'PROPN'),\n ('rausis', 'PROPN'),\n ('michael_adams', 'PROPN'),\n ('alireza', 'NOUN'),\n ('outclass', 'VERB'),\n ('tiviakov', 'PROPN'),\n ('sinquefield', 'PROPN'),\n ('zee', 'PROPN'),\n ('mikhail', 'PROPN'),\n ('rook', 'PROPN'),\n ('koneru', 'PROPN'),\n ('wijk_aan', 'PROPN'),\n ('mcshane', 'PROPN'),\n ('anand', 'PROPN'),\n ('a4', 'PROPN'),\n ('svidler', 'PROPN'),\n ('chess', 'PROPN'),\n ('armenia', 'PROPN'),\n ('bunratty', 'PROPN'),\n ('grünfeld', 'NOUN'),\n ('chess.com', 'X'),\n ('rank_world', 'NOUN'),\n ('biel', 'PROPN'),\n ('chess24.com', 'X'),\n ('tie_break', 'NOUN'),\n ('polgar', 'NOUN'),\n ('final_round', 'PROPN'),\n ('nepomniachtchi', 'PROPN'),\n ('interzonal', 'PROPN'),\n ('checkmate', 'PROPN'),\n ('gelfand', 'PROPN'),\n ('title_challenger', 'PROPN'),\n ('world_title', 'PROPN'),\n ('kazakhstan', 'PROPN'),\n ('chessable', 'PROPN'),\n ('endgame', 'PROPN'),\n ('judit', 'PROPN'),\n ('opening_round', 'PROPN'),\n ('ju', 'PROPN'),\n ('yi', 'PROPN'),\n ('haria', 'PROPN'),\n ('fide', 'PROPN'),\n ('ekaterinburg', 'PROPN'),\n ('outplay', 'PROPN'),\n ('qualifier', 'PROPN'),\n ('wei', 'PROPN'),\n ('xiong', 'PROPN'),\n ('a2', 'PROPN'),\n ('pawn', 'PROPN'),\n ('zagreb', 'PROPN'),\n ('cornishman', 'PROPN'),\n ('lindore', 'NOUN'),\n ('reykjavik', 'PROPN'),\n ('astana', 'PROPN'),\n ('prodigy', 'PROPN'),\n ('alphazero', 'PROPN'),\n ('hao', 'PROPN'),\n ('classical', 'ADJ'),\n ('first_round', 'PROPN'),\n ('chess24', 'PROPN'),\n ('arkady', 'ADV'),\n ('tal', 'VERB'),\n ('pair', 'VERB'),\n ('abidjan', 'PROPN'),\n ('play_off', 'PROPN'),\n ('main_rival', 'PROPN'),\n ('seed', 'NOUN'),\n ('silver_medal', 'PUNCT'),\n ('point_ahead', 'PROPN'),\n ('gambit', 'PROPN'),\n ('azerbaijan', 'PROPN'),\n ('steinitz', 'PROPN'),\n ('quartet', 'NOUN')]"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "[(w.lemma_, w.pos_) for w in nlp(\" \".join(theme_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "''\n",
    "## votes\n",
    "''\n",
    "def av_run(votes: np.array):\n",
    "    \n",
    "    # Expect votes to be a 2D numpy array where each row is a ballot and each column represents a candidate. \n",
    "    # The value of the cell represents the preference order of that voter - lower = preference!\n",
    "\n",
    "    n_ballots, n_candidates = votes.shape\n",
    "\n",
    "    running=True\n",
    "\n",
    "    n_round = 1\n",
    "\n",
    "    losers = set()\n",
    "\n",
    "    while(running):\n",
    "\n",
    "\n",
    "        winners = []\n",
    "\n",
    "        for ballot in votes:\n",
    "            winners.append(np.where(ballot == np.amin(ballot))[0][0])\n",
    "\n",
    "        vote_counts = Counter(winners).most_common()\n",
    "\n",
    "\n",
    "        print(vote_counts)\n",
    "        if vote_counts[0][1] > n_ballots / 2:\n",
    "            print('Winner in round {}. The winner is {}'.format(n_round, vote_counts[0][0]))\n",
    "            running = False\n",
    "            return vote_counts\n",
    "        else:\n",
    "            \n",
    "            loser = next(x[0] for x in reversed(vote_counts) if x[0] not in losers)\n",
    "            votes[:, loser] = n_candidates + 1\n",
    "            print('No winner in round {}. Loser was {}.'.format(n_round, loser))\n",
    "\n",
    "\n",
    "        if n_round > n_candidates:\n",
    "            print('Err! cancelling')        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from math import log\n",
    "\n",
    "def get_doc_word_votes(doc_ids: List[str], words: list, model: Doc2Vec, coeff=1):\n",
    "    \n",
    "    votes = []\n",
    "    \n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        vec = model.docvecs[doc_id];\n",
    "\n",
    "        sims: list = []\n",
    "\n",
    "        for other_word, cnt in words:\n",
    "            other_vec =  model.wv.get_vector(other_word);\n",
    "            sim = abs(model.wv.cosine_similarities(vec, [other_vec])) * (coeff * cnt)\n",
    "\n",
    "            sims.append(sim) \n",
    "            \n",
    "\n",
    "        \n",
    "        sims_ordered = sorted(sims, reverse=True)\n",
    "        sim_indexes = list([sims_ordered.index(sim) for sim in sims])\n",
    "        votes.append(sim_indexes)\n",
    "        \n",
    "\n",
    "    return np.array(votes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr = np.array(articles)\n",
    "doc_arr_trimmed = doc_arr[:len(t)]\n",
    "docs_in_class = doc_arr_trimmed[t == 3]\n",
    "vecs = list([model.docvecs[doc.id] for doc in docs_in_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(model, doc_ids):\n",
    "    words_union = []\n",
    "    for doc_id in doc_ids:\n",
    "        docvec = model.docvecs[doc_id]\n",
    "        words_union += [wc[0] for wc in model.wv.similar_by_vector(docvec, topn=100, restrict_vocab=10000)]\n",
    "    return [wc for wc in Counter(words_union)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n"
    }
   ],
   "source": [
    "doc_ids = [doc.id for doc in docs_in_class]\n",
    "print(len(docs_in_class))\n",
    "words = get_words(model, doc_ids)\n",
    "words_with_counts = [(word, model.wv.vocab[word].count) for word in words]\n",
    "votes = get_doc_word_votes(doc_ids,words_with_counts, model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['family',\n 'struggle',\n 'pay',\n 'funeral',\n 'face',\n 'rise',\n 'price',\n 'country',\n 'help',\n 'council',\n 'net',\n '£',\n 'surplus',\n 'cremation',\n 'cemetery_burial',\n 'observer',\n 'learn',\n 'surplus',\n 'expect',\n 'rise',\n 'significantly',\n 'year',\n 'result',\n 'death_toll',\n 'figure',\n 'more',\n 'authority',\n 'england',\n 'scotland_wales',\n 'obtain_freedom',\n 'information_request',\n 'see',\n 'observer',\n 'show',\n 'average',\n 'rise',\n 'fee',\n 'year',\n '£',\n 'rate_inflation',\n 'big',\n 'increase',\n 'impose',\n 'trafford',\n 'council',\n 'hike',\n '£',\n '£',\n '£',\n 'birmingham_city',\n 'council',\n 'uk',\n 'big',\n 'local_authority',\n 'make',\n 'large',\n 'surplus',\n 'cremation_burial',\n 'total_£',\n 'm',\n 'charge',\n '£',\n 'cremation',\n 'extra',\n '£',\n 'funeral',\n 'overrun',\n 'time',\n 'worthe',\n 'most_expensive',\n 'council',\n 'provide',\n 'datum',\n 'cremation',\n 'charge',\n '£',\n 'increase',\n '£',\n 'low',\n 'charge',\n '£',\n 'south_west',\n 'middlesex',\n 'council',\n 'say',\n 'surplus',\n 'help',\n 'recoup_cost',\n 'invest',\n 'equipment',\n 'upgrade',\n 'meet',\n 'new',\n 'environmental_standard',\n 'spokesperson',\n 'birmingham_city',\n 'council',\n 'say',\n 'operation',\n 'crematorium',\n 'very',\n 'expensive',\n 'business',\n 'investment',\n 'require',\n 'new',\n 'cremator',\n 'require',\n 'pollution',\n 'control',\n 'technology',\n 'can',\n 'cost',\n 'excess_£',\n 'm',\n 'addition',\n 'routine_maintenance',\n 'see',\n 'impact',\n 'funeral',\n 'poverty',\n 'day',\n 'rob',\n 'people',\n 'mental',\n 'space',\n 'grieve',\n 'love_one',\n 'lindesay',\n 'mace',\n 'campaigner',\n 'spokesman',\n 'worthe',\n 'council',\n 'say',\n 'other',\n 'local_authority',\n 'provide',\n 'figure',\n 'charge',\n 'high',\n 'fee',\n 'invest_heavily',\n 'exceptional',\n 'facility',\n 'beautiful',\n 'environment',\n 'work',\n 'hard',\n 'family',\n 'provide',\n 'sensitive',\n 'comforting',\n 'experience',\n 'proud',\n 'service',\n 'provide',\n 'almost_quarter',\n 'local_authority',\n 'provide',\n 'datum',\n 'offer',\n 'direct_cremation',\n 'body',\n 'cremate',\n 'ceremony',\n 'mourner',\n 'allow',\n 'family',\n 'hold',\n 'memorial_service',\n 'time',\n 'place',\n 'own_choosing',\n 'offer',\n 'direct_cremation',\n 'slot',\n 'advertise',\n 'direct_cremation',\n 'cheap',\n 'average',\n '£',\n 'year',\n 'compare',\n '£',\n 'traditional',\n 'cremation',\n 'funeral',\n 'poverty',\n 'campaigner',\n 'say',\n 'direct_cremation',\n 'effective_way',\n 'cut',\n 'cost',\n 'low_income',\n 'family',\n 'earth',\n 'quaker',\n 'social',\n 'action',\n 'organisation',\n 'call',\n 'cap',\n 'cost',\n 'funeral',\n 'say',\n 'nearly',\n 'people',\n 'struggle',\n 'pay',\n 'funeral',\n 'last_year',\n 'lindesay',\n 'mace',\n 'acting',\n 'manager',\n 'say',\n 'see',\n 'impact',\n 'funeral',\n 'poverty',\n 'day',\n 'push',\n 'people',\n 'debt',\n 'rob',\n 'mental',\n 'space',\n 'grieve',\n 'love_one',\n 'competition_market',\n 'authority',\n 'review',\n 'cremation',\n 'fee',\n 'part',\n 'investigation',\n 'funeral',\n 'industry',\n 'could',\n 'recommend',\n 'price_cap',\n 'stop',\n 'above_inflation',\n 'rise',\n 'provisional_finding',\n 'due',\n 'publish',\n 'summer',\n 'steven',\n 'cains',\n 'founder',\n 'harbour',\n 'funeral',\n 'company',\n 'organise',\n 'direct_cremation',\n 'obtain',\n 'datum',\n 'say',\n 'cost',\n 'cremation',\n 'rise',\n 'far',\n 'inflation',\n 'give',\n 'lack',\n 'competition',\n 'barrier_entry',\n 'new',\n 'crematoria',\n 'more',\n 'people',\n 'choose',\n 'direct_cremation',\n 'direct_cremation',\n 'account',\n 'funeral',\n 'expect',\n 'figure',\n 'continue',\n 'rise',\n 'more',\n 'people',\n 'become_disillusioned',\n 'expensive',\n 'hurried',\n 'traditional',\n 'funeral',\n 'service',\n 'say',\n 'high',\n 'know',\n 'council',\n 'cremation',\n 'charge',\n 'worthe',\n '£',\n 'milton_keynes',\n '£',\n 'peterborough',\n '£',\n 'wakefield',\n 'pontefract',\n '£',\n 'bath',\n '£',\n 'inverness',\n '£',\n 'cheltenham',\n '£',\n 'plymouth',\n '£',\n 'dudley',\n '£',\n 'leed',\n '£']"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "docs_in_class[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(0, 9)]\nWinner in round 1. The winner is 0\n"
    }
   ],
   "source": [
    "winner = av_run(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "average_price\n"
    }
   ],
   "source": [
    "for ballot in winner:\n",
    "    print(words[ballot[0]]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr = np.array(articles)\n",
    "doc_arr_trimmed = doc_arr[:len(t)]\n",
    "docs_in_class = doc_arr_trimmed[t == 2]\n",
    "\n",
    "words = []\n",
    "for doc in docs_in_class:\n",
    "    words += list(np.unique(doc.words))\n",
    "top_words = Counter(words).most_common()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['society/2020/jul/25/grieving-families-pushed-into-debt-as-costs-soar-for-burials-and-cremations',\n 'business/2020/may/11/mourners-choosing-simplified-funerals-during-covid-19-crisis',\n 'business/2020/apr/24/funeral-homes-push-for-state-help-as-lockdown-leads-to-no-frills-services',\n 'business/2020/mar/11/dignity-delays-low-cost-funeral-plan-until-after-competition-report',\n 'society/2020/jan/26/church-of-england-could-seek-end-paupers-funerals',\n 'society/2020/jan/06/cost-of-dying-at-record-high-as-price-of-uk-funeral-exceeds-4400',\n 'australia-news/2019/aug/01/funeral-homes-investigation-reveals-high-prices-and-unexplained-charges',\n 'business/2019/may/13/funeral-provider-dignity-warns-fall-in-number-of-deaths-will-hit-profits',\n 'business/2019/mar/28/competition-watchdog-to-investigate-funeral-sector-as-prices-escalate-cma']"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6347216"
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "model.wv.similarity('magnus_carlsen', 'chess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.theme_extractor.article_preprocess_job import ArticlePreprocessJob\n",
    "\n",
    "apj = ArticlePreprocessJob()\n",
    "\n",
    "raw_articles = apj.get_articles_for_latest_load()[:10000]\n",
    "\n",
    "processed_articles = apj.preprocess_raw_articles(raw_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "tokenized_texts = apj.preprocessor.preprocessed_docs\n",
    "\n",
    "\n",
    "def get_ngrams(tokenized_texts: List[List[str]], phrases: Phrases):\n",
    "    return np.unique([token for token in sum([phrases[phrases[tokens]] for tokens in tokenized_texts ], []) if '_' in token])\n",
    "\n",
    "def comp_phrasers():\n",
    "    \n",
    "    phrases1 = Phrases(tokenized_texts)\n",
    "\n",
    "    phrases2 = Phrases(tokenized_texts, scoring='npmi', threshold=0.5, min_count=50)\n",
    "        \n",
    "    tokens1 = get_ngrams(tokenized_texts, phrases1)\n",
    "    tokens2 = get_ngrams(tokenized_texts, phrases2)\n",
    "\n",
    "    \n",
    "    in1butnot2 = np.setdiff1d(tokens1, tokens2, assume_unique=True)\n",
    "    in2butnot1 = np.setdiff1d(tokens2, tokens1, assume_unique=True)\n",
    "\n",
    "    return tokens1, tokens2, in1butnot2, in2butnot1\n",
    "\n",
    "tokens1, tokens2, unique_to_1, unique_to_2 = comp_phrasers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2030\nanti_-\nas_well\nchief_executive\nclimate_change\ncomic_strip\ndonald_trump\nexecutive_order\nhuman_right\nlast_week\nlast_year\nmajority_country\nmuslim_majority\nnew_york\nnon_-\nprime_minister\nsupreme_court\ntell_guardian\ntheresa_may\ntravel_ban\nunited_states\nwhite_house\n£_m\n"
    }
   ],
   "source": [
    "print(len(unique_to_1))\n",
    "for token in tokens2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['ner', 'parser'])\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_html(res: str) -> str:\n",
    "    soup = BeautifulSoup(res, features=\"lxml\")\n",
    "    \n",
    "    for f in soup.find_all('figure'):\n",
    "        f.decompose()\n",
    "    \n",
    "    text = soup.get_text().lower();\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = extract_text_from_html(raw_articles[0].body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['scotland',\n 'yard',\n 'investigate',\n 'claim',\n 'worker',\n 'outsource',\n 'firm',\n 'caput',\n 'pay',\n 'convict',\n 'deliberately',\n 'fit',\n 'electronic',\n 'ankle',\n 'tag',\n 'loosely',\n 'allow',\n 'slip',\n 'device',\n 'when',\n 'want',\n 'go',\n 'staff',\n 'company',\n 'run',\n 'government',\n 'electronic',\n 'monitoring',\n 'service',\n 'allegedly',\n 'pay',\n 'time',\n 'help',\n 'at',\n 'least',\n 'offender',\n 'beat',\n 'court',\n 'impose',\n 'curfew',\n 'accord',\n 'report',\n 'sun',\n 'metropolitan',\n 'police',\n 'say',\n 'investigation',\n 'centre',\n 'london',\n 'borough',\n 'newham',\n 'say',\n 'people',\n 'include',\n 'current',\n 'former',\n 'ems',\n 'worker',\n 'arrest',\n 'connection',\n 'offence',\n 'involve',\n 'monitoring',\n 'offender',\n 'accord',\n 'sun',\n 'scheme',\n 'reveal',\n 'offender',\n 'arrest',\n 'suspicion',\n 'attempt',\n 'murder',\n 'suppose',\n 'home',\n 'curfew',\n 'electronic',\n 'tag',\n 'use',\n 'monitor',\n 'condition',\n 'court',\n 'prison',\n 'order',\n 'usually',\n 'securely',\n 'attach',\n 'ankle',\n 'defender',\n 'can',\n 'remove',\n 'then',\n 'send',\n 'location',\n 'datum',\n 'base',\n 'unit',\n 'offender',\n 'home',\n 'ensure',\n 'remain',\n 'present',\n 'curfew',\n 'hour',\n 'leave',\n 'area',\n 'base',\n 'unit',\n 'send',\n 'alert',\n 'monitor',\n 'centre',\n 'police',\n 'make',\n 'first',\n 'arrest',\n 'case',\n 'january',\n 'when',\n 'hold',\n 'old',\n 'former',\n 'ems',\n 'employee',\n 'romford',\n 'essex',\n 'conspiracy',\n 'pervert',\n 'course',\n 'justice',\n 'theft',\n 'tag',\n 'equipment',\n 'take',\n 'east',\n 'london',\n 'police',\n 'station',\n 'subsequently',\n 'bail',\n 'return',\n 'police',\n 'station',\n 'date',\n 'early',\n 'april',\n 'current',\n 'ems',\n 'worker',\n 'old',\n 'man',\n 'old',\n 'woman',\n 'arrest',\n 'january',\n 'conspiracy',\n 'pervert',\n 'course',\n 'justice',\n 'also',\n 'bail',\n 'return',\n 'date',\n 'early',\n 'april',\n 'further',\n 'people',\n 'none',\n 'employee',\n 'former',\n 'employee',\n 'ems',\n 'arrest',\n 'january',\n 'suspicion',\n 'same',\n 'offence',\n 'capita',\n 'hand',\n 'year',\n 'contract',\n 'electronic',\n 'tagging',\n 'july',\n 'company',\n 'win',\n 'work',\n 'rival',\n 'outsource',\n 'security',\n 'firm',\n 'g4',\n 'serco',\n 'embroil',\n 'overcharge',\n 'allegation',\n 'lead',\n 'repay',\n 'government',\n 'nearly']"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']\n",
    "vals = [token.lemma_ for token in nlp(text) if token.pos_ in allowed_postags and len(token.lemma_) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "p = Phrases([['hello', 'world']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['hello', 'world', 'caramel']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "p[['hello', 'world', 'caramel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}